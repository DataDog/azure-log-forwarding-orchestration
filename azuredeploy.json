{
  "$schema": "https://schema.management.azure.com/schemas/2019-08-01/managementGroupDeploymentTemplate.json#",
  "languageVersion": "2.0",
  "contentVersion": "1.0.0.0",
  "metadata": {
    "_generator": {
      "name": "bicep",
      "version": "0.33.93.31351",
      "templateHash": "7604267253108113197"
    }
  },
  "functions": [
    {
      "namespace": "__bicep",
      "members": {
        "subUuid": {
          "parameters": [
            {
              "type": "string",
              "name": "uuid"
            }
          ],
          "output": {
            "type": "string",
            "value": "[toLower(substring(parameters('uuid'), 24, 12))]"
          }
        }
      }
    }
  ],
  "parameters": {
    "monitoredSubscriptions": {
      "type": "string"
    },
    "controlPlaneLocation": {
      "type": "string"
    },
    "controlPlaneSubscriptionId": {
      "type": "string"
    },
    "controlPlaneResourceGroupName": {
      "type": "string"
    },
    "datadogApplicationKey": {
      "type": "securestring"
    },
    "datadogApiKey": {
      "type": "securestring"
    },
    "datadogSite": {
      "type": "string"
    },
    "datadogTelemetry": {
      "type": "bool",
      "defaultValue": false
    },
    "logLevel": {
      "type": "string",
      "defaultValue": "INFO"
    },
    "imageRegistry": {
      "type": "string",
      "defaultValue": "datadoghq.azurecr.io"
    },
    "storageAccountUrl": {
      "type": "string",
      "defaultValue": "https://ddazurelfo.blob.core.windows.net"
    }
  },
  "variables": {
    "controlPlaneId": "[__bicep.subUuid(guid(managementGroup().id, parameters('controlPlaneSubscriptionId'), parameters('controlPlaneResourceGroupName'), parameters('controlPlaneLocation')))]"
  },
  "resources": {
    "controlPlaneResourceGroup": {
      "type": "Microsoft.Resources/deployments",
      "apiVersion": "2022-09-01",
      "name": "[format('controlPlaneResourceGroup-{0}', variables('controlPlaneId'))]",
      "subscriptionId": "[parameters('controlPlaneSubscriptionId')]",
      "location": "[deployment().location]",
      "properties": {
        "expressionEvaluationOptions": {
          "scope": "inner"
        },
        "mode": "Incremental",
        "parameters": {
          "controlPlaneResourceGroup": {
            "value": "[parameters('controlPlaneResourceGroupName')]"
          },
          "controlPlaneLocation": {
            "value": "[parameters('controlPlaneLocation')]"
          }
        },
        "template": {
          "$schema": "https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#",
          "contentVersion": "1.0.0.0",
          "metadata": {
            "_generator": {
              "name": "bicep",
              "version": "0.33.93.31351",
              "templateHash": "14097886369754460425"
            }
          },
          "parameters": {
            "controlPlaneResourceGroup": {
              "type": "string"
            },
            "controlPlaneLocation": {
              "type": "string"
            }
          },
          "resources": [
            {
              "type": "Microsoft.Resources/resourceGroups",
              "apiVersion": "2021-04-01",
              "name": "[parameters('controlPlaneResourceGroup')]",
              "location": "[parameters('controlPlaneLocation')]"
            }
          ]
        }
      }
    },
    "validateAPIKey": {
      "type": "Microsoft.Resources/deployments",
      "apiVersion": "2022-09-01",
      "name": "[format('validateAPIKey-{0}', variables('controlPlaneId'))]",
      "subscriptionId": "[parameters('controlPlaneSubscriptionId')]",
      "resourceGroup": "[parameters('controlPlaneResourceGroupName')]",
      "properties": {
        "expressionEvaluationOptions": {
          "scope": "inner"
        },
        "mode": "Incremental",
        "parameters": {
          "datadogApiKey": {
            "value": "[parameters('datadogApiKey')]"
          },
          "datadogSite": {
            "value": "[parameters('datadogSite')]"
          }
        },
        "template": {
          "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
          "contentVersion": "1.0.0.0",
          "metadata": {
            "_generator": {
              "name": "bicep",
              "version": "0.33.93.31351",
              "templateHash": "11013346914865890348"
            }
          },
          "parameters": {
            "datadogApiKey": {
              "type": "securestring"
            },
            "datadogSite": {
              "type": "string"
            }
          },
          "resources": [
            {
              "type": "Microsoft.Resources/deploymentScripts",
              "apiVersion": "2020-10-01",
              "name": "validateAPIKeyScript",
              "location": "[resourceGroup().location]",
              "kind": "AzureCLI",
              "properties": {
                "azCliVersion": "2.64.0",
                "environmentVariables": [
                  {
                    "name": "DD_API_KEY",
                    "secureValue": "[parameters('datadogApiKey')]"
                  },
                  {
                    "name": "DD_SITE",
                    "value": "[parameters('datadogSite')]"
                  }
                ],
                "scriptContent": "      tdnf install -y jq\n      response=$(curl -X GET \"https://api.${DD_SITE}/api/v1/validate\" \\\n        -H \"Accept: application/json\" \\\n        -H \"DD-API-KEY: ${DD_API_KEY}\" 2>/dev/null)\n      if [ \"$(jq .valid <<<\"$response\")\" != 'true' ]; then\n        echo \"{\\\"Result\\\": {\\\"error\\\": \\\"Unable to validate API Key against Site '${DD_SITE}'. Please check that the correct Datadog host site was used and that the key is a valid Datadog API key found at https://app.datadoghq.com/organization-settings/api-keys\\\", \\\"response\\\": $response}}\" | jq | tee \"$AZ_SCRIPTS_OUTPUT_PATH\"\n        exit 1\n      fi\n    ",
                "timeout": "PT30M",
                "cleanupPreference": "OnSuccess",
                "retentionInterval": "PT1H"
              }
            }
          ]
        }
      },
      "dependsOn": [
        "controlPlaneResourceGroup"
      ]
    },
    "controlPlane": {
      "type": "Microsoft.Resources/deployments",
      "apiVersion": "2022-09-01",
      "name": "[format('controlPlane-{0}', variables('controlPlaneId'))]",
      "subscriptionId": "[parameters('controlPlaneSubscriptionId')]",
      "resourceGroup": "[parameters('controlPlaneResourceGroupName')]",
      "properties": {
        "expressionEvaluationOptions": {
          "scope": "inner"
        },
        "mode": "Incremental",
        "parameters": {
          "controlPlaneId": {
            "value": "[variables('controlPlaneId')]"
          },
          "controlPlaneLocation": {
            "value": "[parameters('controlPlaneLocation')]"
          },
          "controlPlaneResourceGroupName": {
            "value": "[parameters('controlPlaneResourceGroupName')]"
          },
          "controlPlaneSubscriptionId": {
            "value": "[parameters('controlPlaneSubscriptionId')]"
          },
          "monitoredSubscriptions": {
            "value": "[parameters('monitoredSubscriptions')]"
          },
          "datadogApiKey": {
            "value": "[parameters('datadogApiKey')]"
          },
          "datadogApplicationKey": {
            "value": "[parameters('datadogApplicationKey')]"
          },
          "datadogSite": {
            "value": "[parameters('datadogSite')]"
          },
          "datadogTelemetry": {
            "value": "[parameters('datadogTelemetry')]"
          },
          "imageRegistry": {
            "value": "[parameters('imageRegistry')]"
          },
          "storageAccountUrl": {
            "value": "[parameters('storageAccountUrl')]"
          },
          "logLevel": {
            "value": "[parameters('logLevel')]"
          }
        },
        "template": {
          "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
          "contentVersion": "1.0.0.0",
          "metadata": {
            "_generator": {
              "name": "bicep",
              "version": "0.33.93.31351",
              "templateHash": "11319941780089415655"
            }
          },
          "parameters": {
            "controlPlaneId": {
              "type": "string"
            },
            "controlPlaneLocation": {
              "type": "string"
            },
            "controlPlaneSubscriptionId": {
              "type": "string"
            },
            "controlPlaneResourceGroupName": {
              "type": "string"
            },
            "monitoredSubscriptions": {
              "type": "string"
            },
            "imageRegistry": {
              "type": "string"
            },
            "storageAccountUrl": {
              "type": "string"
            },
            "datadogApiKey": {
              "type": "securestring",
              "metadata": {
                "description": "Datadog API Key"
              }
            },
            "datadogApplicationKey": {
              "type": "securestring",
              "metadata": {
                "description": "Datadog App Key"
              }
            },
            "datadogSite": {
              "type": "string",
              "metadata": {
                "description": "Datadog Site"
              }
            },
            "datadogTelemetry": {
              "type": "bool"
            },
            "logLevel": {
              "type": "string"
            }
          },
          "variables": {
            "$fxv#0": "#!/usr/bin/env bash\n# Bash script intended to be run on the azure-cli:2.65.0 image\nset -euo pipefail\ncurl https://bootstrap.pypa.io/get-pip.py | python3\npip install azure-mgmt-healthcareapis==2.1.0 azure-mgmt-media==10.2.0 azure-mgmt-monitor==6.0.2 azure-mgmt-resource==23.0.1 aiohttp==3.9.5 azure-mgmt-powerbiembedded==3.0.0 tenacity==9.0.0 azure-mgmt-sql==3.0.1 azure-storage-blob==12.20.0 azure-mgmt-netapp==13.3.0 azure-mgmt-synapse==2.0.0 azure-mgmt-appcontainers==3.1.0 aiosonic==0.15.0 azure-mgmt-notificationhubs==8.0.0 azure-mgmt-redisenterprise==3.0.0 azure-mgmt-storage==22.0.0 azure-identity==1.15.0 azure-mgmt-web==7.3.0 jsonschema==4.21.1 datadog-api-client==2.26.0 azure-mgmt-cdn==13.1.1 azure-mgmt-network==28.1.0\npython3 -c '#!/usr/bin/env python\nimport contextlib as __stickytape_contextlib\n\n@__stickytape_contextlib.contextmanager\ndef __stickytape_temporary_dir():\n    import tempfile\n    import shutil\n    dir_path = tempfile.mkdtemp()\n    try:\n        yield dir_path\n    finally:\n        shutil.rmtree(dir_path)\n\nwith __stickytape_temporary_dir() as __stickytape_working_dir:\n    def __stickytape_write_module(path, contents):\n        import os, os.path\n\n        def make_package(path):\n            parts = path.split(\"/\")\n            partial_path = __stickytape_working_dir\n            for part in parts:\n                partial_path = os.path.join(partial_path, part)\n                if not os.path.exists(partial_path):\n                    os.mkdir(partial_path)\n                    with open(os.path.join(partial_path, \"__init__.py\"), \"wb\") as f:\n                        f.write(b\"\\n\")\n\n        make_package(os.path.dirname(path))\n\n        full_path = os.path.join(__stickytape_working_dir, path)\n        with open(full_path, \"wb\") as module_file:\n            module_file.write(contents)\n\n    import sys as __stickytape_sys\n    __stickytape_sys.path.insert(0, __stickytape_working_dir)\n\n    __stickytape_write_module('\"'\"'tasks/__init__.py'\"'\"', b'\"'\"''\"'\"')\n    __stickytape_write_module('\"'\"'tasks/diagnostic_settings_task.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import gather, run\\nfrom random import shuffle\\nfrom typing_extensions import NamedTuple, cast\\n\\n# 3p\\nfrom azure.core.exceptions import HttpResponseError\\nfrom azure.mgmt.monitor.v2021_05_01_preview.aio import MonitorManagementClient\\nfrom azure.mgmt.monitor.v2021_05_01_preview.models import (\\n    CategoryType,\\n    DiagnosticSettingsResource,\\n    LogSettings,\\n)\\n\\n# project\\nfrom cache.assignment_cache import ASSIGNMENT_CACHE_BLOB, deserialize_assignment_cache\\nfrom cache.common import (\\n    InvalidCacheError,\\n    LogForwarderType,\\n)\\nfrom cache.env import CONTROL_PLANE_ID_SETTING, RESOURCE_GROUP_SETTING, get_config_option\\nfrom tasks.common import (\\n    get_event_hub_name,\\n    get_event_hub_namespace,\\n    get_resource_group_id,\\n    get_storage_account_id,\\n)\\nfrom tasks.concurrency import collect\\nfrom tasks.task import Task, task_main\\n\\nDIAGNOSTIC_SETTINGS_TASK_NAME = \"diagnostic_settings_task\"\\n\\nDIAGNOSTIC_SETTING_PREFIX = \"datadog_log_forwarding_\"\\n\\n\\ndef get_authorization_rule_id(sub_id , resource_group , config_id )  :  # pragma: no cover\\n    # no test coverage because we don\\'\"'\"'t have event hub support yet\\n    return (\\n        get_resource_group_id(sub_id, resource_group)\\n        + \"/providers/microsoft.eventhub/namespaces/\"\\n        + get_event_hub_namespace(config_id)\\n        + \"/authorizationrules/rootmanagesharedaccesskey\"\\n    )\\n\\n\\nclass DiagnosticSettingConfiguration(NamedTuple):\\n    \"Convenience Tuple for holding the configuration of a diagnostic setting\"\\n\\n    #id: str\\n    #type: LogForwarderType\\n\\n\\ndef get_diagnostic_setting(\\n    sub_id , resource_group , config , categories )  :\\n    log_settings = [LogSettings(category=category, enabled=True) for category in categories]\\n    if config.type == \"eventhub\":\\n        return DiagnosticSettingsResource(  # pragma: no cover\\n            event_hub_authorization_rule_id=get_authorization_rule_id(sub_id, resource_group, config.id),\\n            event_hub_name=get_event_hub_name(config.id),\\n            logs=log_settings,\\n        )\\n    else:\\n        return DiagnosticSettingsResource(\\n            storage_account_id=get_storage_account_id(sub_id, resource_group, config.id),\\n            logs=log_settings,\\n        )\\n\\n\\nclass DiagnosticSettingsTask(Task):\\n    NAME = DIAGNOSTIC_SETTINGS_TASK_NAME\\n\\n    def __init__(self, assignment_cache_state )  :\\n        super().__init__()\\n\\n        self.resource_group = get_config_option(RESOURCE_GROUP_SETTING)\\n        self.diagnostic_settings_name = (\\n            DIAGNOSTIC_SETTING_PREFIX + get_config_option(CONTROL_PLANE_ID_SETTING)\\n        ).lower()\\n\\n        # read caches\\n        assignment_cache = deserialize_assignment_cache(assignment_cache_state)\\n        if assignment_cache is None:\\n            raise InvalidCacheError(\"Assignment Cache is in an invalid format, failing this task until it is valid\")\\n        self.assignment_cache = assignment_cache\\n\\n    async def run(self)  :\\n        self.log.info(\"Processing %s subscriptions\", len(self.assignment_cache))\\n        await gather(*map(self.process_subscription, self.assignment_cache))\\n\\n    async def process_subscription(self, sub_id )  :\\n        self.log.info(\"Processing subscription %s\", sub_id)\\n        # we assume if a resource isn\\'\"'\"'t in the assignment cache then is has been deleted\\n        async with MonitorManagementClient(self.credential, sub_id) as client:\\n            # TODO: do we want to do anything with management group diagnostic settings?\\n            # client.management_group_diagnostic_settings.list(\"management_group_id\")\\n            resources = [\\n                (resource, DiagnosticSettingConfiguration(config_id, region_config[\"configurations\"][config_id]))\\n                for region_config in self.assignment_cache[sub_id].values()\\n                for resource, config_id in region_config[\"resources\"].items()\\n            ]\\n            shuffle(resources)\\n            await gather(\\n                self.update_subscription_settings(sub_id, client),\\n                *(\\n                    self.process_resource(\\n                        client,\\n                        sub_id,\\n                        resource,\\n                        ds,\\n                    )\\n                    for resource, ds in resources\\n                ),\\n            )\\n\\n    async def update_subscription_settings(self, subscription_id , client )  :\\n        # TODO: do we want to do anything with management group diagnostic settings?\\n        # i.e. client.subscription_diagnostic_settings.list()\\n        return\\n\\n    async def process_resource(\\n        self,\\n        client ,\\n        sub_id ,\\n        resource_id ,\\n        assigned_config ,\\n    )  :\\n        try:\\n            current_diagnostic_settings = await collect(client.diagnostic_settings.list(resource_id))\\n        except HttpResponseError as e:\\n            if e.error and e.error.code and e.error.code.lower() == \"resourcetypenotsupported\":\\n                self.log.warning(\"Resource type for %s unsupported, skipping\", resource_id)\\n                return\\n            self.log.exception(\"Failed to get diagnostic settings for resource %s\", resource_id)\\n            return\\n\\n        current_setting = next(\\n            filter(\\n                lambda ds: cast(str, ds.name).lower() == self.diagnostic_settings_name,\\n                current_diagnostic_settings,\\n            ),\\n            None,\\n        )\\n\\n        if (\\n            current_setting\\n            and current_setting.storage_account_id\\n            and current_setting.storage_account_id.lower()\\n            == get_storage_account_id(sub_id, self.resource_group, assigned_config.id)\\n        ):\\n            return  # it is set up properly already\\n\\n        # otherwise fix it\\n        await self.set_diagnostic_setting(\\n            client,\\n            sub_id,\\n            resource_id,\\n            assigned_config,\\n            # keep the same categories selected (or add all if new), just fix the storage account\\n            categories=current_setting and [cast(str, log.category) for log in (current_setting.logs or [])],\\n        )\\n\\n    async def set_diagnostic_setting(\\n        self,\\n        client ,\\n        sub_id ,\\n        resource_id ,\\n        config ,\\n        categories    = None,\\n    )  :\\n        try:\\n            categories = categories or [\\n                cast(str, category.name)\\n                async for category in client.diagnostic_settings_category.list(resource_id)\\n                if category.category_type == CategoryType.LOGS\\n            ]\\n            if not categories:\\n                self.log.debug(\"No log categories found for resource %s\", resource_id)\\n                return\\n\\n            await client.diagnostic_settings.create_or_update(\\n                resource_id,\\n                self.diagnostic_settings_name,\\n                get_diagnostic_setting(sub_id, self.resource_group, config, categories),\\n            )\\n            self.log.info(\"Added diagnostic setting for resource %s\", resource_id)\\n        except HttpResponseError as e:\\n            if e.error and e.error.code == \"ResourceTypeNotSupported\":\\n                # This resource does not support diagnostic settings\\n                return\\n            if \"Resources should be in the same region\" in str(e):\\n                # todo this should not happen in the real implementation, for now ignore\\n                return\\n            if \"reused in different settings on the same category for the same resource\" in str(e):\\n                self.log.error(\\n                    \"Resource %s already has a diagnostic setting with the same configuration: %s\", resource_id, config\\n                )\\n                return\\n\\n            self.log.error(\"Failed to add diagnostic setting for resource %s -- %s\", resource_id, e.error)\\n        except Exception:\\n            self.log.error(\\n                \"Unexpected error when trying to add diagnostic setting for resource %s\", resource_id, exc_info=True\\n            )\\n\\n    async def write_caches(self)  :\\n        pass  # nothing to do here\\n\\n\\nasync def main()  :\\n    await task_main(DiagnosticSettingsTask, [ASSIGNMENT_CACHE_BLOB])\\n\\n\\nif __name__ == \"__main__\":  # pragma: no cover\\n    run(main())\\n'\"'\"')\n    __stickytape_write_module('\"'\"'cache/__init__.py'\"'\"', b'\"'\"''\"'\"')\n    __stickytape_write_module('\"'\"'cache/assignment_cache.py'\"'\"', b'\"'\"'# stdlib\\nfrom typing_extensions import Any, NotRequired, TypeAlias, TypedDict\\n\\n# 3p\\nfrom jsonschema import ValidationError\\n\\n# project\\nfrom cache.common import LOG_FORWARDER_TYPE_SCHEMA, LogForwarderType, deserialize_cache\\n\\nASSIGNMENT_CACHE_BLOB = \"assignments.json\"\\n\\n\\nclass RegionAssignmentConfiguration(TypedDict, total=True):\\n    #configurations: dict[str, LogForwarderType]\\n    \"Mapping of config_id to DiagnosticSettingType\"\\n    #resources: dict[str, str]\\n    \"Mapping of resource_id to config_id\"\\n    #on_cooldown: NotRequired[bool]\\n    \"whether the region has recently scaled up and is on cooldown\"\\n\\n\\nAssignmentCache  = dict[str, dict[str, RegionAssignmentConfiguration]]\\n\"Mapping of subscription_id to region to RegionAssignmentConfig\"\\n\\n\\nASSIGNMENT_CACHE_SCHEMA   = {\\n    \"type\": \"object\",\\n    \"propertyNames\": {\"format\": \"uuid\"},  # subscription_id\\n    \"additionalProperties\": {\\n        \"type\": \"object\",  # region name\\n        \"additionalProperties\": {\\n            \"properties\": {  # region config\\n                \"configurations\": {\\n                    \"type\": \"object\",  # config_id\\n                    \"additionalProperties\": LOG_FORWARDER_TYPE_SCHEMA,\\n                },\\n                \"resources\": {\\n                    \"type\": \"object\",  # resource_id\\n                    \"additionalProperties\": {\"type\": \"string\"},  # config_id\\n                },\\n                \"on_cooldown\": {\"type\": \"boolean\"},\\n            },\\n            \"required\": [\"configurations\", \"resources\"],\\n            \"additionalProperties\": False,\\n        },\\n    },\\n}\\n\\n\\ndef _validate_valid_config_ids(cache )  :\\n    for region_configs in cache.values():\\n        for region_config in region_configs.values():\\n            configs = region_config[\"configurations\"]\\n            for config_id in region_config[\"resources\"].values():\\n                if config_id not in configs:\\n                    raise ValidationError(f\"Config ID {config_id} not found in region configurations\")\\n    return cache\\n\\n\\ndef deserialize_assignment_cache(cache_str )    :\\n    \"\"\"Deserialize the assignment cache. Returns None if the cache is invalid.\"\"\"\\n    return deserialize_cache(cache_str, ASSIGNMENT_CACHE_SCHEMA, _validate_valid_config_ids)\\n'\"'\"')\n    __stickytape_write_module('\"'\"'cache/common.py'\"'\"', b'\"'\"'# stdlib\\nfrom collections.abc import Callable\\nfrom json import JSONDecodeError, loads\\nfrom logging import DEBUG, getLogger\\nfrom typing_extensions import Any, Final, Literal, NamedTuple, TypeVar\\n\\n# 3p\\nfrom azure.core.exceptions import ResourceNotFoundError\\nfrom azure.storage.blob.aio import BlobClient, StorageStreamDownloader\\nfrom jsonschema import ValidationError, validate\\n\\nfrom cache.env import STORAGE_CONNECTION_SETTING, get_config_option\\n\\nlog = getLogger(__name__)\\nlog.setLevel(DEBUG)\\n\\nT = TypeVar(\"T\")\\n\\nBLOB_STORAGE_CACHE = \"control-plane-cache\"\\n\\nEVENT_HUB_TYPE  = \"eventhub\"\\nSTORAGE_ACCOUNT_TYPE  = \"storageaccount\"\\n\\n\\nLogForwarderType = Literal[\"eventhub\", \"storageaccount\"]\\n\\nLOG_FORWARDER_TYPE_SCHEMA   = {\\n    \"oneOf\": [\\n        {\"const\": STORAGE_ACCOUNT_TYPE},\\n        {\"const\": EVENT_HUB_TYPE},\\n    ],\\n}\\n\\n\\nclass LogForwarder(NamedTuple):\\n    \"\"\"A log forwarder configuration\"\"\"\\n    #config_id: str\\n    #type: LogForwarderType\\n\\n\\nclass InvalidCacheError(Exception):\\n    \"\"\"Raised when the cache is in an invalid state\"\"\"\\n\\n\\nasync def read_cache(blob_name )  :\\n    async with BlobClient.from_connection_string(\\n        get_config_option(STORAGE_CONNECTION_SETTING), BLOB_STORAGE_CACHE, blob_name\\n    ) as blob_client:\\n        try:\\n            blob  = await blob_client.download_blob()\\n        except ResourceNotFoundError:\\n            return \"\"\\n        return (await blob.readall()).decode()\\n\\n\\nasync def write_cache(blob_name , content )  :\\n    async with BlobClient.from_connection_string(\\n        get_config_option(STORAGE_CONNECTION_SETTING), BLOB_STORAGE_CACHE, blob_name\\n    ) as blob_client:\\n        await blob_client.upload_blob(content, overwrite=True)\\n\\n\\ndef deserialize_cache(\\n    cache_str , schema  , post_processing     = lambda x: x\\n)    :\\n    try:\\n        cache = loads(cache_str)\\n        validate(instance=cache, schema=schema)\\n        return post_processing(cache)\\n    except (JSONDecodeError, ValidationError):\\n        return None\\n'\"'\"')\n    __stickytape_write_module('\"'\"'cache/env.py'\"'\"', b'\"'\"'# stdlib\\nfrom collections.abc import Callable\\nfrom logging import getLogger\\nfrom os import environ\\nfrom typing_extensions import TypeVar\\n\\nT = TypeVar(\"T\")\\n\\nlog = getLogger(__name__)\\n\\n\\n# Settings\\nSTORAGE_CONNECTION_SETTING = \"AzureWebJobsStorage\"\\nDD_SITE_SETTING = \"DD_SITE\"\\nDD_API_KEY_SETTING = \"DD_API_KEY\"\\nDD_TELEMETRY_SETTING = \"DD_TELEMETRY\"\\nFORWARDER_IMAGE_SETTING = \"FORWARDER_IMAGE\"\\nSCALING_PERCENTAGE_SETTING = \"SCALING_PERCENTAGE\"\\nCONFIG_ID_SETTING = \"CONFIG_ID\"\\nSUBSCRIPTION_ID_SETTING = \"SUBSCRIPTION_ID\"\\nRESOURCE_GROUP_SETTING = \"RESOURCE_GROUP\"\\nCONTROL_PLANE_REGION_SETTING = \"CONTROL_PLANE_REGION\"\\nCONTROL_PLANE_ID_SETTING = \"CONTROL_PLANE_ID\"\\nMONITORED_SUBSCRIPTIONS_SETTING = \"MONITORED_SUBSCRIPTIONS\"\\nSTORAGE_ACCOUNT_URL_SETTING = \"STORAGE_ACCOUNT_URL\"\\nLOG_LEVEL_SETTING = \"LOG_LEVEL\"\\n\\n# Secret Names\\nDD_API_KEY_SECRET = \"dd-api-key\"\\nCONNECTION_STRING_SECRET = \"connection-string\"\\n\\n\\nclass MissingConfigOptionError(Exception):\\n    def __init__(self, option )  :\\n        super().__init__(f\"Missing required configuration option: {option}\")\\n\\n\\ndef get_config_option(name )  :\\n    \"\"\"Get a configuration option from the environment or raise a helpful error\"\"\"\\n    if option := environ.get(name):\\n        return option\\n    raise MissingConfigOptionError(name)\\n\\n\\ndef parse_config_option(name , parse    , default )  :\\n    \"\"\"Get a configuration option from the environment, parse it, or return a default\"\"\"\\n    try:\\n        value = environ.get(name)\\n        if value is None:\\n            return default\\n        result = parse(value)\\n        if result is None:\\n            log.error(f\"Invalid value for configuration option {name}: {value}\")\\n            return default\\n        return result\\n    except ValueError:\\n        log.error(f\"Invalid value for configuration option {name}: {environ.get(name)}\")\\n        return default\\n\\n\\ndef is_truthy(setting_name )  :\\n    return environ.get(setting_name, \"\").lower().strip() in {\"t\", \"true\", \"1\", \"y\", \"yes\"}\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/common.py'\"'\"', b'\"'\"'# stdlib\\nfrom collections.abc import Iterable, Mapping\\nfrom datetime import datetime\\nfrom logging import Logger\\nfrom math import inf\\nfrom typing_extensions import Final, Protocol, TypeVar\\nfrom uuid import uuid4\\n\\nCONTROL_PLANE_APP_SERVICE_PLAN_PREFIX  = \"dd-lfo-control-\"\\nCONTROL_PLANE_STORAGE_ACCOUNT_PREFIX  = \"ddlfocontrol\"\\nSCALING_TASK_PREFIX  = \"scaling-task-\"\\nRESOURCES_TASK_PREFIX  = \"resources-task-\"\\nDIAGNOSTIC_SETTINGS_TASK_PREFIX  = \"diagnostic-settings-task-\"\\n\\n\\nFORWARDER_CONTAINER_APP_PREFIX  = \"dd-log-forwarder-\"\\nFORWARDER_MANAGED_ENVIRONMENT_PREFIX  = \"dd-log-forwarder-env-\"\\nFORWARDER_STORAGE_ACCOUNT_PREFIX  = \"ddlogstorage\"\\n\\n# TODO We will need to add prefixes for these when we implement event hub support\\nEVENT_HUB_NAME_PREFIX  = NotImplemented\\nEVENT_HUB_NAMESPACE_PREFIX  = NotImplemented\\n\\n\\ndef get_container_app_name(config_id )  :\\n    return FORWARDER_CONTAINER_APP_PREFIX + config_id\\n\\n\\ndef get_resource_group_id(subscription_id , resource_group )  :\\n    return f\"/subscriptions/{subscription_id}/resourcegroups/{resource_group}\".lower()\\n\\n\\ndef get_container_app_id(subscription_id , resource_group , config_id )  :\\n    return (\\n        get_resource_group_id(subscription_id, resource_group)\\n        + \"/providers/microsoft.app/jobs/\"\\n        + get_container_app_name(config_id)\\n    ).lower()\\n\\n\\ndef get_managed_env_name(region , control_plane_id )  :\\n    return f\"{FORWARDER_MANAGED_ENVIRONMENT_PREFIX}{control_plane_id}-{region}\"\\n\\n\\ndef get_managed_env_id(subscription_id , resource_group , region , control_plane_id )  :\\n    return (\\n        get_resource_group_id(subscription_id, resource_group)\\n        + \"/providers/microsoft.app/managedenvironments/\"\\n        + get_managed_env_name(region, control_plane_id)\\n    ).lower()\\n\\n\\ndef get_storage_account_name(config_id )  :\\n    return FORWARDER_STORAGE_ACCOUNT_PREFIX + config_id\\n\\n\\ndef get_storage_account_id(subscription_id , resource_group , config_id )  :\\n    return (\\n        get_resource_group_id(subscription_id, resource_group)\\n        + \"/providers/microsoft.storage/storageaccounts/\"\\n        + get_storage_account_name(config_id)\\n    ).lower()\\n\\n\\ndef get_event_hub_name(config_id )  :  # pragma: no cover\\n    return EVENT_HUB_NAME_PREFIX + config_id  # type: ignore\\n\\n\\ndef get_event_hub_namespace(config_id )  :  # pragma: no cover\\n    return EVENT_HUB_NAMESPACE_PREFIX + config_id  # type: ignore\\n\\n\\ndef now()  :\\n    \"\"\"Return the current time in ISO format\"\"\"\\n    return datetime.now().isoformat()\\n\\n\\ndef average(*items , default  = inf)  :\\n    \"\"\"Return the average of the items, or `default` if no items are provided\"\"\"\\n    if not items:\\n        return default\\n    return sum(items) / len(items)\\n\\n\\nT = TypeVar(\"T\")\\n\\n\\ndef generate_unique_id()  :\\n    \"\"\"Generate a unique ID which is 12 characters long using hex characters\\n\\n    Example:\\n    >>> generate_unique_id()\\n    \"c5653797a664\"\\n    \"\"\"\\n    return str(uuid4())[-12:]\\n\\n\\ndef chunks(lst , n )   :\\n    \"\"\"Yield successive n-sized chunks from lst. If the last chunk is smaller than n, it will be dropped\"\"\"\\n    return zip(*(lst[i::n] for i in range(n)), strict=False)\\n\\n\\ndef log_errors(\\n    log ,\\n    message ,\\n    *maybe_errors   ,\\n    reraise  = False,\\n    extra     = None,\\n)  :\\n    \"\"\"Log and return any errors in `maybe_errors`.\\n    If reraise is True, the first error will be raised\"\"\"\\n    errors = [e for e in maybe_errors if isinstance(e, Exception)]\\n    if errors:\\n        log.exception(\"%s: %s\", message, errors, extra=extra)\\n        if reraise:\\n            raise errors[0]\\n\\n    return errors\\n\\n\\nclass Resource(Protocol):\\n    \"\"\"Azure resource names are a string, useful for type casting\"\"\"\\n\\n    #name: str\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/concurrency.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import Task, create_task\\nfrom collections.abc import AsyncIterable, Awaitable\\nfrom logging import Logger\\nfrom typing_extensions import TypeVar\\n\\nT = TypeVar(\"T\")\\n\\n\\nasync def collect(it )  :\\n    \"\"\"Helper for collecting an async iterable, useful for simplifying error handling\"\"\"\\n    return [item async for item in it]\\n\\n\\nasync def safe_collect(it , log )  :\\n    \"\"\"Helper for collecting an async iterable, logs a warning if an error was thrown but returns the result so far\"\"\"\\n    collected = []\\n    try:\\n        async for item in it:\\n            collected.append(item)\\n    except Exception as e:\\n        log.warning(\"Ignored error while collecting async iterable: %s\", e)\\n    return collected\\n\\n\\ndef create_task_from_awaitable(awaitable )  :\\n    \"\"\"Turns an awaitable object into an asyncio Task,\\n    which starts it immediately and it can be awaited later\"\"\"\\n\\n    async def _f()  :\\n        return await awaitable\\n\\n    return create_task(_f())\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/task.py'\"'\"', b'\"'\"'# stdlib\\nfrom abc import abstractmethod\\nfrom asyncio import create_task, gather\\nfrom contextlib import AbstractAsyncContextManager\\nfrom datetime import datetime, timezone\\nfrom logging import ERROR, Handler, LogRecord, basicConfig, getLogger\\nfrom os import environ\\nfrom traceback import format_exception\\nfrom types import TracebackType\\nfrom typing_extensions import Self\\nfrom uuid import uuid4\\n\\n# 3p\\nfrom azure.identity.aio import DefaultAzureCredential\\nfrom datadog_api_client import AsyncApiClient, Configuration\\nfrom datadog_api_client.v2.api.logs_api import LogsApi\\nfrom datadog_api_client.v2.model.http_log import HTTPLog\\nfrom datadog_api_client.v2.model.http_log_item import HTTPLogItem\\n\\n# project\\nfrom cache.common import read_cache\\nfrom cache.env import CONTROL_PLANE_ID_SETTING, DD_API_KEY_SETTING, DD_TELEMETRY_SETTING, LOG_LEVEL_SETTING, is_truthy\\nfrom tasks.common import now\\n\\nlog = getLogger(__name__)\\n\\n# silence azure logging except for errors\\ngetLogger(\"azure\").setLevel(ERROR)\\n\\nIGNORED_LOG_EXTRAS = {\"created\", \"relativeCreated\", \"thread\", \"args\", \"msg\"}\\n\\n\\ndef get_error_telemetry(\\n    exc_info           ,\\n)   :\\n    telemetry = {}\\n    if not exc_info:\\n        return telemetry\\n    exc_type, exc, tb = exc_info\\n    if exc_type:\\n        telemetry[\"exception\"] = exc_type.__name__\\n    if exc_type or exc or tb:\\n        telemetry[\"exc_info\"] = \"\".join(format_exception(exc_type, value=exc, tb=tb, limit=20))\\n    return telemetry\\n\\n\\nclass ListHandler(Handler):\\n    \"\"\"A logging handler that appends log messages to a list\"\"\"\\n\\n    def __init__(self, logs ):\\n        super().__init__()\\n        self.log_list = logs\\n\\n    def emit(self, record )  :\\n        record.asctime = datetime.now(timezone.utc).isoformat()\\n        self.log_list.append(record)\\n\\n\\nclass Task(AbstractAsyncContextManager[\"Task\"]):\\n    #NAME: str\\n\\n    def __init__(self)  :\\n        self.credential = DefaultAzureCredential()\\n\\n        # Telemetry Logic\\n        self.execution_id = str(uuid4())\\n        self.control_plane_id = environ.get(CONTROL_PLANE_ID_SETTING, \"unknown\")\\n        tags = [\"forwarder:lfocontrolplane\", f\"task:{self.NAME}\", f\"control_plane_id:{self.control_plane_id}\"]\\n        self.dd_tags = \",\".join(tags)\\n        self.telemetry_enabled = bool(is_truthy(DD_TELEMETRY_SETTING) and environ.get(DD_API_KEY_SETTING))\\n        self.log = log.getChild(self.__class__.__name__)\\n        self._logs  = []\\n        self._datadog_client = AsyncApiClient(Configuration())\\n        self._logs_client = LogsApi(self._datadog_client)\\n        if self.telemetry_enabled:\\n            log.info(\"Telemetry enabled, will submit logs for %s\", self.NAME)\\n            self.log.addHandler(ListHandler(self._logs))\\n\\n    @abstractmethod\\n    async def run(self)  : ...\\n\\n    async def __aenter__(self)  :\\n        await gather(self.credential.__aenter__(), self._datadog_client.__aenter__())\\n        return self\\n\\n    async def __aexit__(\\n        self, exc_type   , exc_value   , traceback       )  :\\n        submit_telemetry = create_task(self.submit_telemetry())\\n        if exc_type is None and exc_value is None and traceback is None:\\n            await self.write_caches()\\n        await self.credential.__aexit__(exc_type, exc_value, traceback)\\n        await submit_telemetry\\n        await self._datadog_client.__aexit__(exc_type, exc_value, traceback)\\n\\n    @abstractmethod\\n    async def write_caches(self)  : ...\\n\\n    async def submit_telemetry(self)  :\\n        if not self.telemetry_enabled or not self._logs:\\n            return\\n        dd_logs = [\\n            HTTPLogItem(\\n                **{\\n                    **{k: str(v) for k, v in record.__dict__.items() if k not in IGNORED_LOG_EXTRAS},\\n                    **{\\n                        \"message\": record.getMessage(),\\n                        \"ddsource\": \"azure\",\\n                        \"service\": \"lfo\",\\n                        \"time\": record.asctime,\\n                        \"level\": record.levelname,\\n                        \"execution_id\": self.execution_id,\\n                        \"control_plane_id\": self.control_plane_id,\\n                        \"task\": self.NAME,\\n                    },\\n                    **get_error_telemetry(record.exc_info),\\n                }\\n            )\\n            for record in self._logs\\n        ]\\n        self._logs.clear()\\n        await self._logs_client.submit_log(HTTPLog(value=dd_logs), ddtags=self.dd_tags)  # type: ignore\\n\\n\\nasync def task_main(task_class , caches )  :\\n    level = environ.get(LOG_LEVEL_SETTING, \"INFO\").upper()\\n    if level not in {\"ERROR\", \"WARN\", \"WARNING\", \"INFO\", \"DEBUG\"}:\\n        level = \"INFO\"\\n    basicConfig()\\n    log.setLevel(level)\\n    log.info(\"Started %s at %s (log level %s)\", task_class.NAME, now(), level)\\n    cache_states = await gather(*map(read_cache, caches))\\n    async with task_class(*cache_states) as task:\\n        await task.run()\\n    log.info(\"%s finished at %s\", task_class.NAME, now())\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/resources_task.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import gather, run\\nfrom json import dumps\\nfrom os import getenv\\nfrom typing_extensions import cast\\n\\n# 3p\\nfrom azure.mgmt.resource.subscriptions.v2021_01_01.aio import SubscriptionClient\\n\\n# project\\nfrom cache.common import write_cache\\nfrom cache.env import MONITORED_SUBSCRIPTIONS_SETTING\\nfrom cache.resources_cache import (\\n    RESOURCE_CACHE_BLOB,\\n    ResourceCache,\\n    deserialize_monitored_subscriptions,\\n    deserialize_resource_cache,\\n    prune_resource_cache,\\n)\\nfrom tasks.client.resource_client import ResourceClient\\nfrom tasks.task import Task, task_main\\n\\nRESOURCES_TASK_NAME = \"resources_task\"\\n\\n\\nclass ResourcesTask(Task):\\n    NAME = RESOURCES_TASK_NAME\\n\\n    def __init__(self, resource_cache_state )  :\\n        super().__init__()\\n        self.monitored_subscriptions = deserialize_monitored_subscriptions(\\n            getenv(MONITORED_SUBSCRIPTIONS_SETTING) or \"\"\\n        )\\n        resource_cache = deserialize_resource_cache(resource_cache_state)\\n        if resource_cache is None:\\n            self.log.warning(\"Resource Cache is in an invalid format, task will reset the cache\")\\n            resource_cache = {}\\n        self._resource_cache_initial_state = resource_cache\\n\\n        self.resource_cache  = {}\\n        \"in-memory cache of subscription_id to resource_ids\"\\n\\n    async def run(self)  :\\n        async with SubscriptionClient(self.credential) as subscription_client:\\n            subscriptions = [\\n                cast(str, sub.subscription_id).lower() async for sub in subscription_client.subscriptions.list()\\n            ]\\n\\n        self.log.info(\"Found %s subscriptions\", len(subscriptions))\\n\\n        if self.monitored_subscriptions is not None:\\n            all_subscription_count = len(subscriptions)\\n            subscriptions = [sub for sub in subscriptions if sub in self.monitored_subscriptions]\\n            self.log.info(\\n                \"Filtered %s subscriptions down to the monitored subscriptions list (%s subscriptions)\",\\n                all_subscription_count,\\n                len(subscriptions),\\n            )\\n\\n        await gather(*map(self.process_subscription, subscriptions))\\n\\n    async def process_subscription(self, subscription_id )  :\\n        self.log.debug(\"Processing the following subscription: %s\", subscription_id)\\n        async with ResourceClient(self.log, self.credential, subscription_id) as client:\\n            self.resource_cache[subscription_id] = await client.get_resources_per_region()\\n\\n    async def write_caches(self)  :\\n        prune_resource_cache(self.resource_cache)\\n\\n        subscription_count = len(self.resource_cache)\\n        region_count = sum(len(regions) for regions in self.resource_cache.values())\\n        resources_count = sum(\\n            len(resources) for regions in self.resource_cache.values() for resources in regions.values()\\n        )\\n\\n        if self.resource_cache == self._resource_cache_initial_state:\\n            self.log.info(\"Resources have not changed, no update needed to %s resources\", resources_count)\\n            return\\n\\n        # since sets cannot be json serialized, we convert them to lists before storing\\n        await write_cache(RESOURCE_CACHE_BLOB, dumps(self.resource_cache, default=list))\\n\\n        self.log.info(\\n            \"Updated Resources, monitoring %s resources stored in the cache across %s regions across %s subscriptions\",\\n            resources_count,\\n            region_count,\\n            subscription_count,\\n        )\\n\\n\\nasync def main()  :\\n    await task_main(ResourcesTask, [RESOURCE_CACHE_BLOB])\\n\\n\\nif __name__ == \"__main__\":  # pragma: no cover\\n    run(main())\\n'\"'\"')\n    __stickytape_write_module('\"'\"'cache/resources_cache.py'\"'\"', b'\"'\"'# stdlib\\nfrom typing_extensions import Any, TypeAlias\\n\\n# 3p\\nfrom cache.common import deserialize_cache\\n\\nRESOURCE_CACHE_BLOB = \"resources.json\"\\n\\n\\nMONITORED_SUBSCRIPTIONS_SCHEMA   = {\\n    \"type\": \"array\",\\n    \"items\": {\"type\": \"string\"},\\n}\\n\\n\\ndef deserialize_monitored_subscriptions(env_str )    :\\n    return deserialize_cache(env_str, MONITORED_SUBSCRIPTIONS_SCHEMA, lambda subs: [sub.lower() for sub in subs])\\n\\n\\nResourceCache  = dict[str, dict[str, set[str]]]\\n\"mapping of subscription_id to region to resource_ids\"\\n\\n\\nRESOURCE_CACHE_SCHEMA   = {\\n    \"type\": \"object\",\\n    \"propertyNames\": {\"format\": \"uuid\"},\\n    \"additionalProperties\": {\"type\": \"object\", \"additionalProperties\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}},\\n}\\n\\n\\ndef deserialize_resource_cache(cache_str )    :\\n    \"\"\"Deserialize the resource cache. Returns None if the cache is invalid.\"\"\"\\n\\n    def convert_resources_to_set(cache )  :\\n        for resources_per_region in cache.values():\\n            for region in resources_per_region:\\n                resources_per_region[region] = set(resources_per_region[region])\\n        return cache\\n\\n    return deserialize_cache(cache_str, RESOURCE_CACHE_SCHEMA, convert_resources_to_set)\\n\\n\\ndef prune_resource_cache(cache )  :\\n    \"\"\"Prune the cache by removing empty regions and subscriptions.\"\"\"\\n    for subscription_id, resources_per_region in list(cache.items()):\\n        for region, resources in list(resources_per_region.items()):\\n            if not resources:\\n                del resources_per_region[region]\\n        if not resources_per_region:\\n            del cache[subscription_id]\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/client/resource_client.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import gather\\nfrom collections.abc import AsyncGenerator, AsyncIterable, Callable, Iterable, Mapping\\nfrom contextlib import AbstractAsyncContextManager\\nfrom itertools import chain\\nfrom logging import Logger\\nfrom types import TracebackType\\nfrom typing_extensions import Any, Final, Protocol, Self, TypeAlias, cast\\n\\n# 3p\\nfrom azure.identity.aio import DefaultAzureCredential\\nfrom azure.mgmt.cdn.aio import CdnManagementClient\\nfrom azure.mgmt.core.tools import parse_resource_id\\nfrom azure.mgmt.healthcareapis.aio import HealthcareApisManagementClient\\nfrom azure.mgmt.media.aio import AzureMediaServices\\nfrom azure.mgmt.netapp.aio import NetAppManagementClient\\nfrom azure.mgmt.network.aio import NetworkManagementClient\\nfrom azure.mgmt.notificationhubs.aio import NotificationHubsManagementClient\\nfrom azure.mgmt.powerbiembedded.aio import PowerBIEmbeddedManagementClient\\nfrom azure.mgmt.redisenterprise.aio import RedisEnterpriseManagementClient\\nfrom azure.mgmt.resource.resources.v2021_01_01.aio import ResourceManagementClient\\nfrom azure.mgmt.resource.resources.v2021_01_01.models import GenericResourceExpanded\\nfrom azure.mgmt.sql.aio import SqlManagementClient\\nfrom azure.mgmt.synapse.aio import SynapseManagementClient\\nfrom azure.mgmt.web.v2023_12_01.aio import WebSiteManagementClient\\n\\n# project\\nfrom tasks.common import (\\n    CONTROL_PLANE_STORAGE_ACCOUNT_PREFIX,\\n    DIAGNOSTIC_SETTINGS_TASK_PREFIX,\\n    FORWARDER_MANAGED_ENVIRONMENT_PREFIX,\\n    FORWARDER_STORAGE_ACCOUNT_PREFIX,\\n    RESOURCES_TASK_PREFIX,\\n    SCALING_TASK_PREFIX,\\n)\\nfrom tasks.concurrency import safe_collect\\nfrom tasks.constants import (\\n    ALLOWED_STORAGE_ACCOUNT_REGIONS,\\n    FETCHED_RESOURCE_TYPES,\\n    NESTED_VALID_RESOURCE_TYPES,\\n    UNNESTED_VALID_RESOURCE_TYPES,\\n)\\n\\nRESOURCE_QUERY_FILTER  = \" or \".join(f\"resourceType eq \\'\"'\"'{rt}\\'\"'\"'\" for rt in FETCHED_RESOURCE_TYPES)\\n\\n# we only need to ignore forwardable resource types here, since others will be filtered by type.\\nIGNORED_LFO_PREFIXES  = frozenset(\\n    {\\n        FORWARDER_MANAGED_ENVIRONMENT_PREFIX,\\n        SCALING_TASK_PREFIX,\\n        RESOURCES_TASK_PREFIX,\\n        DIAGNOSTIC_SETTINGS_TASK_PREFIX,\\n        CONTROL_PLANE_STORAGE_ACCOUNT_PREFIX,\\n        FORWARDER_STORAGE_ACCOUNT_PREFIX,\\n    }\\n)\\n\\n\\nFetchSubResources  = Callable[[GenericResourceExpanded], AsyncIterable[str]]\\n\\n\\nclass SDKClientMethod(Protocol):\\n    def __call__(self, resource_group , resource_name , /, **kwargs )  : ...\\n\\n\\nasync def get_storage_account_services(r )  :\\n    for service_type in NESTED_VALID_RESOURCE_TYPES[\"microsoft.storage/storageaccounts\"]:\\n        yield f\"{r.id}/{service_type}/default\".lower()\\n\\n\\ndef safe_get_id(r )    :\\n    if hasattr(r, \"id\") and isinstance(r.id, str):\\n        return r.id.lower()\\n    return None\\n\\n\\ndef should_ignore_resource(region , resource_type , resource_name )  :\\n    \"\"\"Determines if we should ignore the resource\"\"\"\\n    name = resource_name.lower()\\n    return (\\n        # we must be able to put a storage account in the same region\\n        # https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings#destination-limitations\\n        region.lower() not in ALLOWED_STORAGE_ACCOUNT_REGIONS\\n        # ignore resources that are managed by the control plane\\n        or any(name.startswith(prefix) for prefix in IGNORED_LFO_PREFIXES)\\n        # only certain resource types have diagnostic settings, this is a confirmation that the filter worked\\n        or resource_type.lower() not in FETCHED_RESOURCE_TYPES\\n    )\\n\\n\\nclass ResourceClient(AbstractAsyncContextManager[\"ResourceClient\"]):\\n    def __init__(self, log , cred , subscription_id )  :\\n        super().__init__()\\n        self.log = log\\n        self.credential = cred\\n        self.subscription_id = subscription_id\\n        self.resources_client = ResourceManagementClient(cred, subscription_id)\\n        redis_client = RedisEnterpriseManagementClient(cred, subscription_id)\\n        cdn_client = CdnManagementClient(cred, subscription_id)\\n        healthcareapis_client = HealthcareApisManagementClient(cred, subscription_id)\\n        media_client = AzureMediaServices(cred, subscription_id)\\n        network_client = NetworkManagementClient(cred, subscription_id)\\n        netapp_client = NetAppManagementClient(cred, subscription_id)\\n        notificationhubs_client = NotificationHubsManagementClient(cred, subscription_id)\\n        powerbi_client = PowerBIEmbeddedManagementClient(cred, subscription_id)\\n        sql_client = SqlManagementClient(cred, subscription_id)\\n        synapse_client = SynapseManagementClient(cred, subscription_id)\\n        web_client = WebSiteManagementClient(cred, subscription_id)\\n\\n        # map of resource type to client and sub resource fetching function\\n        self._get_sub_resources_map                          = {\\n            \"microsoft.cache/redisenterprise\": (\\n                redis_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(redis_client.databases.list_by_cluster),\\n            ),\\n            \"microsoft.cdn/profiles\": (\\n                cdn_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(cdn_client.endpoints.list_by_profile),\\n            ),\\n            \"microsoft.healthcareapis/workspaces\": (\\n                healthcareapis_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(\\n                    healthcareapis_client.dicom_services.list_by_workspace,\\n                    healthcareapis_client.fhir_services.list_by_workspace,\\n                    healthcareapis_client.iot_connectors.list_by_workspace,\\n                ),\\n            ),\\n            \"microsoft.media/mediaservices\": (\\n                media_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(\\n                    media_client.live_events.list, media_client.streaming_endpoints.list\\n                ),\\n            ),\\n            \"microsoft.netapp/netappaccounts\": (\\n                netapp_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(netapp_client.pools.list),\\n            ),\\n            \"microsoft.network/networkmanagers\": (\\n                network_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(network_client.ipam_pools.list),\\n            ),\\n            \"microsoft.notificationhubs/namespaces\": (\\n                notificationhubs_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(notificationhubs_client.notification_hubs.list),\\n            ),\\n            \"microsoft.powerbi/tenants\": (\\n                powerbi_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(powerbi_client.workspaces.list),\\n            ),\\n            \"microsoft.storage/storageaccounts\": (None, get_storage_account_services),\\n            \"microsoft.sql/servers\": (\\n                sql_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(sql_client.databases.list_by_server),\\n            ),\\n            \"microsoft.sql/managedinstances\": (\\n                None,\\n                self.make_sub_resource_extractor_for_rg_and_name(sql_client.managed_databases.list_by_instance),\\n            ),\\n            \"microsoft.synapse/workspaces\": (\\n                synapse_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(\\n                    synapse_client.big_data_pools.list_by_workspace,\\n                    synapse_client.sql_pools.list_by_workspace,\\n                ),\\n            ),\\n            \"microsoft.web/sites\": (\\n                web_client,\\n                self.make_sub_resource_extractor_for_rg_and_name(web_client.web_apps.list_slots),\\n            ),\\n        }\\n\\n\\n\\n    def make_sub_resource_extractor_for_rg_and_name(self, *functions )  :\\n        \"\"\"Creates an extractor for sub resource IDs based on the resource group and name\"\"\"\\n\\n        async def _f(r )  :\\n            resource_group = getattr(r, \"resource_group\", None)\\n            resource_name = r.name\\n            if not isinstance(resource_group, str) or not isinstance(\\n                resource_name, str\\n            ):  # fallback to parsing the resource id\\n                parsed = parse_resource_id(cast(str, r.id))\\n                resource_group = cast(str, parsed[\"resource_group\"])\\n                resource_name = cast(str, parsed[\"name\"])\\n            self.log.debug(\"Extracting sub resources for %s\", r.id)\\n            sub_resources = await gather(\\n                *(safe_collect(f(resource_group, resource_name, timeout=30), self.log) for f in functions)\\n            )\\n            for sub_resource in chain.from_iterable(sub_resources):\\n                if hasattr(sub_resource, \"value\") and isinstance(sub_resource.value, Iterable):\\n                    for resource in sub_resource.value:\\n                        if rid := safe_get_id(resource):\\n                            yield rid\\n                elif rid := safe_get_id(sub_resource):\\n                    yield rid\\n\\n        return _f\\n\\n    async def __aenter__(self)  :\\n        await gather(\\n            self.resources_client.__aenter__(),\\n            *(client.__aenter__() for client, _ in self._get_sub_resources_map.values() if client is not None),\\n        )\\n        return self\\n\\n    async def __aexit__(\\n        self, exc_type   , exc_val   , exc_tb       )  :\\n        await gather(\\n            self.resources_client.__aexit__(exc_type, exc_val, exc_tb),\\n            *(\\n                client.__aexit__(exc_type, exc_val, exc_tb)\\n                for client, _ in self._get_sub_resources_map.values()\\n                if client is not None\\n            ),\\n        )\\n\\n    async def get_resources_per_region(self)   :\\n        resources_per_region   = {}\\n\\n        resources = await safe_collect(self.resources_client.resources.list(RESOURCE_QUERY_FILTER), self.log)\\n        valid_resources = [\\n            r\\n            for r in resources\\n            if not should_ignore_resource(cast(str, r.location), cast(str, r.type), cast(str, r.name))\\n        ]\\n        self.log.debug(\\n            \"Collected %s valid resources for subscription %s, fetching sub-resources...\",\\n            len(valid_resources),\\n            self.subscription_id,\\n        )\\n        batched_resource_ids = await gather(\\n            *(safe_collect(self.all_resource_ids_for_resource(r), self.log) for r in valid_resources)\\n        )\\n        for resource, resource_ids in zip(valid_resources, batched_resource_ids, strict=False):\\n            region = cast(str, resource.location).lower()\\n            resources_per_region.setdefault(region, set()).update(resource_ids)\\n\\n        self.log.info(\\n            \"Subscription %s: Collected %s resources\",\\n            self.subscription_id,\\n            sum(len(rs) for rs in resources_per_region.values()),\\n        )\\n        return resources_per_region\\n\\n    async def all_resource_ids_for_resource(self, resource )  :\\n        resource_id = cast(str, resource.id).lower()\\n        resource_type = cast(str, resource.type).lower()\\n        if resource_type in UNNESTED_VALID_RESOURCE_TYPES:\\n            yield resource_id\\n        if resource_type in self._get_sub_resources_map:\\n            _, get_sub_resources = self._get_sub_resources_map[resource_type]\\n            async for sub_resource in get_sub_resources(resource):\\n                yield sub_resource\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/constants.py'\"'\"', b'\"'\"'from collections.abc import Mapping\\nfrom itertools import chain\\nfrom typing_extensions import Final\\n\\n# https://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-logs/logs-index\\nUNNESTED_VALID_RESOURCE_TYPES  = frozenset(\\n    {\\n        \"microsoft.aad/domainservices\",\\n        \"microsoft.agfoodplatform/farmbeats\",\\n        \"microsoft.analysisservices/servers\",\\n        \"microsoft.apimanagement/service\",\\n        \"microsoft.app/managedenvironments\",\\n        \"microsoft.appconfiguration/configurationstores\",\\n        \"microsoft.appplatform/spring\",\\n        \"microsoft.attestation/attestationproviders\",\\n        \"microsoft.automation/automationaccounts\",\\n        \"microsoft.autonomousdevelopmentplatform/accounts\",\\n        \"microsoft.autonomousdevelopmentplatform/workspaces\",\\n        \"microsoft.avs/privateclouds\",\\n        \"microsoft.azureplaywrightservice/accounts\",\\n        \"microsoft.azuresphere/catalogs\",\\n        \"microsoft.batch/batchaccounts\",\\n        \"microsoft.botservice/botservices\",\\n        \"microsoft.cache/redis\",\\n        \"microsoft.cdn/cdnwebapplicationfirewallpolicies\",\\n        \"microsoft.cdn/profiles\",\\n        \"microsoft.chaos/experiments\",\\n        \"microsoft.classicnetwork/networksecuritygroups\",\\n        \"microsoft.codesigning/codesigningaccounts\",\\n        \"microsoft.cognitiveservices/accounts\",\\n        \"microsoft.communication/communicationservices\",\\n        \"microsoft.community/communitytrainings\",\\n        \"microsoft.compute/virtualmachines\",\\n        \"microsoft.confidentialledger/managedccf\",\\n        \"microsoft.confidentialledger/managedccfs\",\\n        \"microsoft.connectedcache/cachenodes\",\\n        \"microsoft.connectedcache/enterprisemcccustomers\",\\n        \"microsoft.connectedcache/ispcustomers\",\\n        \"microsoft.connectedvehicle/platformaccounts\",\\n        \"microsoft.containerinstance/containergroups\",\\n        \"microsoft.containerregistry/registries\",\\n        \"microsoft.containerservice/fleets\",\\n        \"microsoft.containerservice/managedclusters\",\\n        \"microsoft.customproviders/resourceproviders\",\\n        \"microsoft.d365customerinsights/instances\",\\n        \"microsoft.dashboard/grafana\",\\n        \"microsoft.databricks/workspaces\",\\n        \"microsoft.datafactory/factories\",\\n        \"microsoft.datalakeanalytics/accounts\",\\n        \"microsoft.datalakestore/accounts\",\\n        \"microsoft.dataprotection/backupvaults\",\\n        \"microsoft.datashare/accounts\",\\n        \"microsoft.dbformariadb/servers\",\\n        \"microsoft.dbformysql/flexibleservers\",\\n        \"microsoft.dbformysql/servers\",\\n        \"microsoft.dbforpostgresql/flexibleservers\",\\n        \"microsoft.dbforpostgresql/servergroupsv2\",\\n        \"microsoft.dbforpostgresql/servers\",\\n        \"microsoft.dbforpostgresql/serversv2\",\\n        \"microsoft.desktopvirtualization/appattachpackages\",\\n        \"microsoft.desktopvirtualization/applicationgroups\",\\n        \"microsoft.desktopvirtualization/hostpools\",\\n        \"microsoft.desktopvirtualization/scalingplans\",\\n        \"microsoft.desktopvirtualization/workspaces\",\\n        \"microsoft.devcenter/devcenters\",\\n        \"microsoft.devices/iothubs\",\\n        \"microsoft.devices/provisioningservices\",\\n        \"microsoft.devopsinfrastructure/pools\",\\n        \"microsoft.digitaltwins/digitaltwinsinstances\",\\n        \"microsoft.documentdb/cassandraclusters\",\\n        \"microsoft.documentdb/databaseaccounts\",\\n        \"microsoft.documentdb/mongoclusters\",\\n        \"microsoft.eventgrid/domains\",\\n        \"microsoft.eventgrid/namespaces\",\\n        \"microsoft.eventgrid/partnernamespaces\",\\n        \"microsoft.eventgrid/partnertopics\",\\n        \"microsoft.eventgrid/systemtopics\",\\n        \"microsoft.eventgrid/topics\",\\n        \"microsoft.eventhub/namespaces\",\\n        \"microsoft.hardwaresecuritymodules/cloudhsmclusters\",\\n        \"microsoft.healthcareapis/services\",\\n        \"microsoft.healthdataaiservices/deidservices\",\\n        \"microsoft.insights/autoscalesettings\",\\n        \"microsoft.insights/components\",\\n        \"microsoft.insights/datacollectionrules\",\\n        \"microsoft.keyvault/managedhsms\",\\n        \"microsoft.keyvault/vaults\",\\n        \"microsoft.kubernetes/connectedclusters\",\\n        \"microsoft.kusto/clusters\",\\n        \"microsoft.loadtestservice/loadtests\",\\n        \"microsoft.logic/integrationaccounts\",\\n        \"microsoft.logic/workflows\",\\n        \"microsoft.machinelearningservices/registries\",\\n        \"microsoft.machinelearningservices/workspaces\",\\n        \"microsoft.managednetworkfabric/networkdevices\",\\n        \"microsoft.media/mediaservices\",\\n        \"microsoft.media/videoanalyzers\",\\n        \"microsoft.monitor/accounts\",\\n        \"microsoft.network/applicationgateways\",\\n        \"microsoft.network/azurefirewalls\",\\n        \"microsoft.network/bastionhosts\",\\n        \"microsoft.network/dnsresolverpolicies\",\\n        \"microsoft.network/expressroutecircuits\",\\n        \"microsoft.network/frontdoors\",\\n        \"microsoft.network/loadbalancers\",\\n        \"microsoft.network/networkmanagers\",\\n        \"microsoft.network/networksecuritygroups\",\\n        \"microsoft.network/networksecurityperimeters\",\\n        \"microsoft.network/p2svpngateways\",\\n        \"microsoft.network/publicipaddresses\",\\n        \"microsoft.network/publicipprefixes\",\\n        \"microsoft.network/trafficmanagerprofiles\",\\n        \"microsoft.network/virtualnetworkgateways\",\\n        \"microsoft.network/virtualnetworks\",\\n        \"microsoft.network/vpngateways\",\\n        \"microsoft.networkanalytics/dataproducts\",\\n        \"microsoft.networkcloud/baremetalmachines\",\\n        \"microsoft.networkcloud/clustermanagers\",\\n        \"microsoft.networkcloud/clusters\",\\n        \"microsoft.networkcloud/storageappliances\",\\n        \"microsoft.networkfunction/azuretrafficcollectors\",\\n        \"microsoft.notificationhubs/namespaces\",\\n        \"microsoft.openenergyplatform/energyservices\",\\n        \"microsoft.openlogisticsplatform/workspaces\",\\n        \"microsoft.operationalinsights/workspaces\",\\n        \"microsoft.playfab/titles\",\\n        \"microsoft.powerbi/tenants\",\\n        \"microsoft.powerbidedicated/capacities\",\\n        \"microsoft.providerhub/providermonitorsettings\",\\n        \"microsoft.providerhub/providerregistrations\",\\n        \"microsoft.purview/accounts\",\\n        \"microsoft.recoveryservices/vaults\",\\n        \"microsoft.relay/namespaces\",\\n        \"microsoft.search/searchservices\",\\n        \"microsoft.security/antimalwaresettings\",\\n        \"microsoft.security/defenderforstoragesettings\",\\n        \"microsoft.servicebus/namespaces\",\\n        \"microsoft.servicenetworking/trafficcontrollers\",\\n        \"microsoft.signalrservice/signalr\",\\n        \"microsoft.signalrservice/webpubsub\",\\n        \"microsoft.singularity/accounts\",\\n        \"microsoft.sql/managedinstances\",\\n        \"microsoft.storagecache/amlfilesystems\",\\n        \"microsoft.storagecache/caches\",\\n        \"microsoft.storagemover/storagemovers\",\\n        \"microsoft.streamanalytics/streamingjobs\",\\n        \"microsoft.synapse/workspaces\",\\n        \"microsoft.videoindexer/accounts\",\\n        \"microsoft.web/hostingenvironments\",\\n        \"microsoft.web/sites\",\\n        \"microsoft.web/staticsites\",\\n        \"microsoft.workloads/sapvirtualinstances\",\\n        \"nginx.nginxplus/nginxdeployment\",\\n    }\\n)\\n\\n# resource type to their sub-types\\nNESTED_VALID_RESOURCE_TYPES   = {\\n    # \"microsoft.azuredatatransfer/connections\": {\"flows\"}, TODO(AZINTS-3054)\\n    \"microsoft.cache/redisenterprise\": {\"databases\"},\\n    \"microsoft.cdn/profiles\": {\"endpoints\"},\\n    \"microsoft.healthcareapis/workspaces\": {\"dicomservices\", \"fhirservices\", \"iotconnectors\"},\\n    # \"microsoft.machinelearningservices/workspaces\": {\"onlineendpoints\"}, TODO(AZINTS-3054)\\n    \"microsoft.media/mediaservices\": {\"liveevents\", \"streamingendpoints\"},\\n    \"microsoft.netapp/netappaccounts\": {\\n        \"capacitypools\",\\n        # \"capacitypools/volumes\" TODO(AZINTS-3055)\\n    },\\n    # \"microsoft.network/networksecurityperimeters\": {\"profiles\"}, TODO(AZINTS-3054)\\n    \"microsoft.network/networkmanagers\": {\"ipampools\"},\\n    \"microsoft.notificationhubs/namespaces\": {\"notificationhubs\"},\\n    \"microsoft.powerbi/tenants\": {\"workspaces\"},\\n    # \"microsoft.signalrservice/signalr\": {\"replicas\"}, TODO(AZINTS-3054)\\n    # \"microsoft.signalrservice/webpubsub\": {\"replicas\"}, TODO(AZINTS-3054)\\n    \"microsoft.storage/storageaccounts\": {\"blobservices\", \"fileservices\", \"queueservices\", \"tableservices\"},\\n    \"microsoft.sql/servers\": {\"databases\"},\\n    \"microsoft.sql/managedinstances\": {\"databases\"},\\n    \"microsoft.synapse/workspaces\": {\\n        \"bigdatapools\",\\n        \"sqlpools\",\\n        # \"kustopools\", TODO(AZINTS-3054)\\n        # \"scopepools\", TODO(AZINTS-3054)\\n    },\\n    \"microsoft.web/sites\": {\"slots\"},\\n}\\n\\nFETCHED_RESOURCE_TYPES  = frozenset(chain(UNNESTED_VALID_RESOURCE_TYPES, NESTED_VALID_RESOURCE_TYPES))\\n\\n\\n# Azure Product Region Availability\\n# Overview: https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/table\\n# To ensure an up to date list, attempt to create the resources in a region which does not exist, and check the error message\\n\\nALLOWED_STORAGE_ACCOUNT_REGIONS  = frozenset(\\n    {\\n        \"australiacentral\",\\n        \"australiaeast\",\\n        \"australiasoutheast\",\\n        \"brazilsouth\",\\n        \"canadacentral\",\\n        \"canadaeast\",\\n        \"centralindia\",\\n        \"centralus\",\\n        \"eastasia\",\\n        \"eastus\",\\n        \"eastus2\",\\n        \"francecentral\",\\n        \"germanywestcentral\",\\n        \"israelcentral\",\\n        \"italynorth\",\\n        \"japaneast\",\\n        \"japanwest\",\\n        \"jioindiawest\",\\n        \"koreacentral\",\\n        \"koreasouth\",\\n        \"mexicocentral\",\\n        \"newzealandnorth\",\\n        \"northcentralus\",\\n        \"northeurope\",\\n        \"norwayeast\",\\n        \"polandcentral\",\\n        \"qatarcentral\",\\n        \"southafricanorth\",\\n        \"southcentralus\",\\n        \"southeastasia\",\\n        \"southindia\",\\n        \"spaincentral\",\\n        \"swedencentral\",\\n        \"switzerlandnorth\",\\n        \"uaenorth\",\\n        \"uksouth\",\\n        \"ukwest\",\\n        \"westcentralus\",\\n        \"westeurope\",\\n        \"westindia\",\\n        \"westus\",\\n        \"westus2\",\\n        \"westus3\",\\n    }\\n)\\n\\nALLOWED_CONTAINER_APP_REGIONS  = frozenset(\\n    {\\n        \"australiaeast\",\\n        \"brazilsouth\",\\n        \"canadacentral\",\\n        \"canadaeast\",\\n        \"centralindia\",\\n        \"centralus\",\\n        \"eastasia\",\\n        \"eastus\",\\n        \"eastus2\",\\n        \"francecentral\",\\n        \"germanywestcentral\",\\n        \"italynorth\",\\n        \"japaneast\",\\n        \"koreacentral\",\\n        \"northcentralus\",\\n        \"northeurope\",\\n        \"norwayeast\",\\n        \"polandcentral\",\\n        \"southafricanorth\",\\n        \"southcentralus\",\\n        \"southeastasia\",\\n        \"southindia\",\\n        \"swedencentral\",\\n        \"switzerlandnorth\",\\n        \"uaenorth\",\\n        \"uksouth\",\\n        \"ukwest\",\\n        \"westcentralus\",\\n        \"westeurope\",\\n        \"westus\",\\n        \"westus2\",\\n        \"westus3\",\\n    }\\n)\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/scaling_task.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import gather, run\\nfrom collections.abc import Generator, Iterable\\nfrom copy import deepcopy\\nfrom datetime import datetime, timedelta\\nfrom itertools import chain\\nfrom json import dumps\\nfrom typing_extensions import cast\\n\\n# 3p\\nfrom azure.mgmt.appcontainers.models import Job\\nfrom tenacity import retry, retry_if_result, stop_after_attempt\\n\\n# project\\nfrom cache.assignment_cache import (\\n    ASSIGNMENT_CACHE_BLOB,\\n    AssignmentCache,\\n    RegionAssignmentConfiguration,\\n    deserialize_assignment_cache,\\n)\\nfrom cache.common import (\\n    InvalidCacheError,\\n    LogForwarder,\\n    write_cache,\\n)\\nfrom cache.env import (\\n    CONTROL_PLANE_REGION_SETTING,\\n    RESOURCE_GROUP_SETTING,\\n    SCALING_PERCENTAGE_SETTING,\\n    get_config_option,\\n    parse_config_option,\\n)\\nfrom cache.metric_blob_cache import MetricBlobEntry\\nfrom cache.resources_cache import RESOURCE_CACHE_BLOB, ResourceCache, deserialize_resource_cache\\nfrom tasks.client.log_forwarder_client import LogForwarderClient\\nfrom tasks.common import average, chunks, generate_unique_id, log_errors\\nfrom tasks.constants import ALLOWED_CONTAINER_APP_REGIONS\\nfrom tasks.task import Task, task_main\\n\\nSCALING_TASK_NAME = \"scaling_task\"\\n\\nSCALING_METRIC_PERIOD_MINUTES = 5\\nDELETION_METRIC_PERIOD_MINUTES = 15\\nMETRIC_COLLECTION_PERIOD_MINUTES = DELETION_METRIC_PERIOD_MINUTES  # longer of the two periods^\\nDEFAULT_SCALING_PERCENTAGE = 0.8\\n\\nSCALE_UP_EXECUTION_SECONDS = 45\\nSCALE_DOWN_EXECUTION_SECONDS = 3\\n\\nMAX_FORWARDERS_PER_REGION = 15\\n\\n\\ndef is_consistently_over_threshold(metrics , threshold , percentage )  :\\n    \"\"\"Check if the runtime is consistently over the threshold.\\n    percentage is a float between 0 and 1, representing the percentage of metrics that need to exceed the threshold\\n    \"\"\"\\n    if not metrics:\\n        return False\\n    exceeded_metrics = [metric for metric in metrics if metric[\"runtime_seconds\"] > threshold]\\n    return float(len(exceeded_metrics)) / len(metrics) > percentage\\n\\n\\ndef is_consistently_under_threshold(metrics , threshold , percentage )  :\\n    \"\"\"Check if the runtime is consistently under the threshold\\n    percentage is a float between 0 and 1, representing the percentage of metrics that need to be under the threshold\\n    \"\"\"\\n    if not metrics:\\n        return False\\n    under_threshold_metrics = [metric for metric in metrics if metric[\"runtime_seconds\"] < threshold]\\n    return float(len(under_threshold_metrics)) / len(metrics) > percentage\\n\\n\\ndef resources_to_move_by_load(resource_loads  )    :\\n    half_load = sum(resource_loads.values()) / 2\\n    load_so_far = 0\\n\\n    def _sort_key(kv  )   :\\n        \"\"\"Sort by load, then alphabetically if we have a tie\"\"\"\\n        return kv[1], kv[0]\\n\\n    for resource, load in sorted(resource_loads.items(), key=_sort_key):\\n        load_so_far += load\\n        if load_so_far > half_load:\\n            yield resource\\n\\n\\ndef prune_assignment_cache(resource_cache , assignment_cache )  :\\n    \"\"\"Updates the assignment cache based on any deletions in the resource cache\"\"\"\\n    assignment_cache = deepcopy(assignment_cache)\\n\\n    def _prune_region_config(subscription_id , region )  :\\n        resources = resource_cache.get(subscription_id, {}).get(region, set())\\n        current_region_config = assignment_cache.get(subscription_id, {}).get(\\n            region,\\n            {\"configurations\": {}, \"resources\": {}},  # default empty region config\\n        )\\n        current_region_config[\"resources\"] = {\\n            resource_id: config_id\\n            for resource_id in resources\\n            if (config_id := current_region_config[\"resources\"].get(resource_id))\\n        }\\n        return current_region_config\\n\\n    pruned_cache = {\\n        sub_id: {region: _prune_region_config(sub_id, region) for region in region_resources}\\n        for sub_id, region_resources in resource_cache.items()\\n    }\\n\\n    # add any regions that are in the assignment cache but not in the resource cache\\n    for sub_id, region_configs in assignment_cache.items():\\n        for region, config in region_configs.items():\\n            if region not in pruned_cache.get(sub_id, {}):\\n                # clear just the resources, we still have forwarders to clean up\\n                pruned_cache.setdefault(sub_id, {})[region] = {**config, \"resources\": {}}\\n\\n    return pruned_cache\\n\\n\\nclass ScalingTask(Task):\\n    NAME = SCALING_TASK_NAME\\n\\n    def __init__(self, resource_cache_state , assignment_cache_state )  :\\n        super().__init__()\\n        self.resource_group = get_config_option(RESOURCE_GROUP_SETTING)\\n        self.scaling_percentage = parse_config_option(SCALING_PERCENTAGE_SETTING, float, DEFAULT_SCALING_PERCENTAGE)\\n        self.control_plane_region = get_config_option(CONTROL_PLANE_REGION_SETTING)\\n\\n        self.now = datetime.now()\\n\\n        # Resource Cache\\n        resource_cache = deserialize_resource_cache(resource_cache_state)\\n        if resource_cache is None:\\n            raise InvalidCacheError(\"Resource Cache is in an invalid format, failing this task until it is valid\")\\n        self.resource_cache = resource_cache\\n\\n        # Assignment Cache\\n        assignment_cache = deserialize_assignment_cache(assignment_cache_state)\\n        if assignment_cache is None:\\n            self.log.warning(\"Assignment Cache is in an invalid format, task will reset the cache\")\\n            assignment_cache = {}\\n        self._assignment_cache_initial_state = assignment_cache\\n        self.assignment_cache = prune_assignment_cache(resource_cache, assignment_cache)\\n\\n    async def run(self)  :\\n        self.log.info(\"Running for %s subscriptions: %s\", len(self.resource_cache), list(self.resource_cache.keys()))\\n        all_subscriptions = set(self.resource_cache.keys()) | set(self._assignment_cache_initial_state.keys())\\n        await gather(*(self.process_subscription(sub_id) for sub_id in all_subscriptions))\\n\\n    async def process_subscription(self, subscription_id )  :\\n        regions_with_forwarders = {\\n            region\\n            for region, region_config in self._assignment_cache_initial_state.get(subscription_id, {}).items()\\n            if region_config.get(\"configurations\")\\n        }\\n        regions_with_resources = set(self.resource_cache.get(subscription_id, {}).keys())\\n        # regions with any forwarders or environments\\n        provisioned_regions = set(self._assignment_cache_initial_state.get(subscription_id, {}).keys())\\n\\n        regions_to_add = regions_with_resources - regions_with_forwarders\\n        regions_to_remove = provisioned_regions - regions_with_resources\\n        regions_to_check_scaling = regions_with_resources & regions_with_forwarders\\n        async with LogForwarderClient(self.log, self.credential, subscription_id, self.resource_group) as client:\\n            await gather(\\n                *(self.set_up_region(client, subscription_id, region) for region in regions_to_add),\\n                *(self.delete_region(client, subscription_id, region) for region in regions_to_remove),\\n                *(\\n                    self.maintain_existing_region(client, subscription_id, region)\\n                    for region in regions_to_check_scaling\\n                ),\\n            )\\n            await self.clean_up_orphaned_forwarders(client, subscription_id)\\n\\n    @retry(stop=stop_after_attempt(3), retry=retry_if_result(lambda result: result is None))\\n    async def create_log_forwarder(self, client , region )    :\\n        \"\"\"Creates a log forwarder for the given subscription and region and returns the configuration id and type.\\n        Will try 3 times, and if the creation fails, the forwarder is (attempted to be) deleted and None is returned.\\n        If container apps are not supported in the region, the forwarder is created in the same region as the control plane.\"\"\"\\n        config_id = generate_unique_id()\\n        try:\\n            config_type = await client.create_log_forwarder(region, config_id)\\n            return LogForwarder(config_id, config_type)\\n        except Exception:\\n            self.log.exception(\"Failed to create log forwarder %s, cleaning up\", config_id)\\n            success = await client.delete_log_forwarder(config_id, raise_error=False)\\n            if not success:\\n                self.log.error(\"Failed to clean up log forwarder %s, manual intervention required\", config_id)\\n            return None\\n\\n    async def create_log_forwarder_env(self, client , region )  :\\n        \"\"\"Creates a log forwarder env for the given region. If the creation fails, the env is (attempted to be) deleted\"\"\"\\n        try:\\n            await client.create_log_forwarder_managed_environment(region)\\n        except Exception:\\n            self.log.exception(\"Failed to create log forwarder env for region %s, cleaning up\", region)\\n            success = await client.delete_log_forwarder_env(region, raise_error=False)\\n            if not success:\\n                self.log.error(\\n                    \"Failed to clean up log forwarder env for region %s, manual intervention required\", region\\n                )\\n\\n    async def set_up_region(\\n        self,\\n        client ,\\n        subscription_id ,\\n        region ,\\n    )  :\\n        \"\"\"Creates a log forwarder for the given subscription and region and assigns resources to it.\\n        Only done the first time we discover a new region.\\n\\n        Will never raise an exception.\\n        \"\"\"\\n        env_exists = await self.check_region_forwarder_env(client, region)\\n        if not env_exists:\\n            self.log.info(\"Creating log forwarder env for subscription %s in region %s\", subscription_id, region)\\n            await self.create_log_forwarder_env(client, region)\\n            # log forwarder environments take multiple minutes to be ready, so we should wait until the next run\\n            return\\n\\n        log_forwarder = await self.create_log_forwarder(client, region)\\n        if log_forwarder is None:\\n            return\\n        config_id, config_type = log_forwarder\\n        self.assignment_cache.setdefault(subscription_id, {})[region] = {\\n            \"configurations\": {config_id: config_type},\\n            \"resources\": {resource: config_id for resource in self.resource_cache[subscription_id][region]},\\n        }\\n        await self.write_caches()\\n\\n    async def delete_region(\\n        self,\\n        client ,\\n        subscription_id ,\\n        region ,\\n    )  :\\n        \"\"\"Cleans up a region by deleting all log forwarders for the given subscription and region.\"\"\"\\n        forwarders = self._assignment_cache_initial_state[subscription_id][region][\"configurations\"]\\n        if not forwarders:\\n            # never delete control plane region env, it is either used by the deployer task or potentially unsupported regions\\n            # if the region is supported by container apps, it has its own env and should be deleted\\n            if region != self.control_plane_region and region in ALLOWED_CONTAINER_APP_REGIONS:\\n                self.log.info(\\n                    \"Deleting log forwarder managed env for subscription %s in region %s\", subscription_id, region\\n                )\\n                await client.delete_log_forwarder_env(region, raise_error=False)\\n\\n            self.assignment_cache.get(subscription_id, {}).pop(region, None)\\n            await self.write_caches()\\n            return\\n\\n        # not needed, but useful to indicate all resources are gone\\n        self.assignment_cache[subscription_id][region][\"resources\"].clear()\\n\\n        forwarder_metrics = await self.collect_region_forwarder_metrics(client, forwarders)\\n        forwarders_to_delete = [\\n            forwarder\\n            for forwarder, metrics in forwarder_metrics.items()\\n            if not any(m[\"resource_log_volume\"] for m in metrics)\\n        ]\\n        if not forwarders_to_delete:\\n            self.log.info(\"Attempted to delete region %s but all forwarders are still receiving logs\", region)\\n            return\\n\\n        self.log.info(\"Deleting log forwarders for subscription %s in region %s\", subscription_id, region)\\n        maybe_errors = await gather(\\n            *(\\n                self.delete_log_forwarder(client, self.assignment_cache[subscription_id][region], forwarder_id)\\n                for forwarder_id in forwarders_to_delete\\n            ),\\n            return_exceptions=True,\\n        )\\n        log_errors(self.log, \"Failed to delete region\", *maybe_errors)\\n        await self.write_caches()\\n\\n    async def maintain_existing_region(self, client , subscription_id , region )  :\\n        \"\"\"Checks the performance/scaling of a region and determines/performs scaling as needed\\n\\n        Additionally assigns new resources to the least busy forwarder\\n        and reassigns resources based on the new scaling,\\n        as well as ensuring existing forwarders have up to date settings\"\"\"\\n        self.log.info(\"Checking scaling for log forwarders in subscription %s in region %s\", subscription_id, region)\\n        region_config = self.assignment_cache[subscription_id][region]\\n\\n        env_exists = await self.check_region_forwarder_env(client, region)\\n        if not env_exists:\\n            self.log.error(\\n                \"Log forwarder env missing for subscription %s in region %s. Setting up new one.\",\\n                subscription_id,\\n                region,\\n            )\\n            await self.create_log_forwarder_env(client, region)\\n            return\\n\\n        all_forwarders_exist = await self.ensure_region_forwarders(client, subscription_id, region)\\n        if not all_forwarders_exist:\\n            return\\n        if region_config.get(\"on_cooldown\", False):\\n            self.log.info(\"Region %s is on cooldown, skipping scaling this run\", region)\\n            del region_config[\"on_cooldown\"]\\n            return\\n        all_forwarder_metrics = await self.collect_region_forwarder_metrics(client, region_config[\"configurations\"])\\n        if not any(all_forwarder_metrics.values()):\\n            self.log.warning(\"No valid metrics found for forwarders in region %s\", region)\\n            return\\n\\n        self.onboard_new_resources(subscription_id, region, all_forwarder_metrics)\\n        await self.write_caches()\\n\\n        # count the number of resources after we have onboarded new resources\\n        config_ids = list(region_config[\"resources\"].values())\\n        num_resources_by_forwarder = {\\n            config_id: config_ids.count(config_id) for config_id in region_config[\"configurations\"]\\n        }\\n        scaling_metric_cutoff = (self.now - timedelta(minutes=METRIC_COLLECTION_PERIOD_MINUTES)).timestamp()\\n        scaling_forwarder_metrics = {\\n            f: [m for m in metrics if m[\"timestamp\"] > scaling_metric_cutoff]\\n            for f, metrics in all_forwarder_metrics.items()\\n        }\\n\\n        did_scale = await self.scale_up_forwarders(\\n            client, subscription_id, region, num_resources_by_forwarder, scaling_forwarder_metrics\\n        )\\n        await self.write_caches()\\n        # if we don\\'\"'\"'t scale up, we can check for scaling down\\n        if did_scale:\\n            return\\n\\n        await self.scale_down_forwarders(\\n            client, region, region_config, num_resources_by_forwarder, scaling_forwarder_metrics, all_forwarder_metrics\\n        )\\n        await self.write_caches()\\n\\n    async def ensure_region_forwarders(self, client , subscription_id , region )  :\\n        \"\"\"Ensures that all forwarders cache still exist, making the necessary adjustments to `self.assignment_cache`\\n        if they don\\'\"'\"'t. Returns True if all forwarders exist, False if there are issues\\n\\n        ASSUMPTION: Assignment cache is pruned before we execute this. (see `prune_assignment_cache`)\"\"\"\\n        region_config = self.assignment_cache[subscription_id][region]\\n        if not region_config[\"configurations\"]:\\n            # TODO(AZINTS-2968) we should do as little as possible, probably just exit out and clear the cache\\n            self.log.warning(\"No forwarders found in cache for region %s, recreating\", region)\\n            self.assignment_cache[subscription_id].pop(region)\\n            await self.write_caches()\\n            return False\\n        # fetch log resources\\n        forwarder_resources_list = await gather(\\n            *(client.get_forwarder_resources(config_id) for config_id in region_config[\"configurations\"])\\n        )\\n        forwarder_resources = dict(zip(region_config[\"configurations\"], forwarder_resources_list, strict=False))\\n\\n        if all(all(resources) for resources in forwarder_resources.values()):\\n            # everything is there! check that the forwarder settings are correct\\n            errors = await gather(\\n                *(\\n                    self.ensure_forwarder_settings(client, region, config_id, cast(Job, job))\\n                    for config_id, (job, _) in forwarder_resources.items()\\n                ),\\n                return_exceptions=True,\\n            )\\n            log_errors(self.log, \"Failed to ensure forwarder settings are updated\", *errors)\\n            return True\\n\\n        # if all forwarders have been deleted, we should delete the region from the cache and exit\\n        # it will be recreated next time\\n        if all(not all(resources) for resources in forwarder_resources.values()):\\n            self.log.warning(\"All forwarders gone in region %s\", region)\\n            self.assignment_cache[subscription_id].pop(region)\\n            await self.write_caches()\\n            return False\\n\\n        # if there are some partially missing forwarders, delete them from\\n        # the cache and move them over to an existing forwarder\\n        broken_forwarders = {\\n            config_id for config_id, forwarder_resources in forwarder_resources.items() if not all(forwarder_resources)\\n        }\\n        working_forwarder = next(\\n            config_id for config_id, forwarder_resources in forwarder_resources.items() if all(forwarder_resources)\\n        )\\n        region_config[\"resources\"].update(\\n            {\\n                resource_id: working_forwarder\\n                for resource_id, config_id in region_config[\"resources\"].items()\\n                if config_id in broken_forwarders\\n            }\\n        )\\n        for broken_forwarder in broken_forwarders:\\n            region_config[\"configurations\"].pop(broken_forwarder)\\n\\n        await self.write_caches()\\n        return False\\n\\n    async def ensure_forwarder_settings(\\n        self, client , region , config_id , forwarder     )  :\\n        \"\"\"Ensures that the forwarder has the correct settings, updating them if not\"\"\"\\n        expected_secrets = await client.generate_forwarder_secrets(config_id)\\n        expected_settings = client.generate_forwarder_settings(config_id)\\n        if (\\n            forwarder.template\\n            and forwarder.template.containers\\n            and forwarder.configuration\\n            and forwarder.configuration.secrets\\n        ):\\n            actual_secrets_by_name = {secret.name: secret.value for secret in forwarder.configuration.secrets}\\n            expected_secrets_by_name = {secret.name: secret.value for secret in expected_secrets}\\n            actual_settings_by_name = {\\n                setting.name: setting.as_dict() for setting in forwarder.template.containers[0].env or []\\n            }\\n            expected_settings_by_name = {setting.name: setting.as_dict() for setting in expected_settings}\\n            if (\\n                expected_secrets_by_name == actual_secrets_by_name\\n                and expected_settings_by_name == actual_settings_by_name\\n            ):\\n                return  # everything looks good!\\n\\n        self.log.info(\"Updating settings for forwarder %s\", config_id)\\n        # just await the begin_update call, don\\'\"'\"'t poll for it to finish\\n        await client.create_or_update_log_forwarder_container_app(\\n            region, config_id, env=expected_settings, secrets=expected_secrets\\n        )\\n\\n    async def check_region_forwarder_env(self, client , region )  :\\n        \"\"\"Checks to see if the forwarder env exists for a given region\"\"\"\\n        return bool(await client.get_log_forwarder_managed_environment(region))\\n\\n    async def collect_region_forwarder_metrics(\\n        self, client , log_forwarders     )   :\\n        \"\"\"Collects metrics for all forwarders in a region and returns them as a dictionary by config_id. Returns an empty dict on failure.\"\"\"\\n        oldest_metric_timestamp = (self.now - timedelta(minutes=METRIC_COLLECTION_PERIOD_MINUTES)).timestamp()\\n        maybe_metrics = await gather(\\n            *(client.collect_forwarder_metrics(config_id, oldest_metric_timestamp) for config_id in log_forwarders),\\n            return_exceptions=True,\\n        )\\n        errors = log_errors(self.log, \"Failed to collect metrics for forwarders\", *maybe_metrics, reraise=False)\\n        if errors:\\n            return {}\\n        return dict(zip(log_forwarders, cast(list[list[MetricBlobEntry]], maybe_metrics), strict=False))\\n\\n    def onboard_new_resources(\\n        self, subscription_id , region , forwarder_metrics      )  :\\n        \"\"\"Assigns new resources to the least busy forwarder in the region, and updates the cache state accordingly\"\"\"\\n        new_resources = set(self.resource_cache[subscription_id][region]) - set(\\n            self.assignment_cache[subscription_id][region][\"resources\"]\\n        )\\n        if not new_resources:\\n            return\\n\\n        least_busy_forwarder_id = min(\\n            # any forwarders without metrics we should not add more resources to, there may be something wrong:\\n            filter(lambda forwarder_id: forwarder_metrics[forwarder_id], forwarder_metrics),\\n            # find forwarder with the min average runtime\\n            key=lambda forwarder_id: average(\\n                *(metric[\"runtime_seconds\"] for metric in forwarder_metrics[forwarder_id])\\n            ),\\n        )\\n\\n        self.assignment_cache[subscription_id][region][\"resources\"].update(\\n            {resource: least_busy_forwarder_id for resource in new_resources}\\n        )\\n\\n    async def scale_up_forwarders(\\n        self,\\n        client ,\\n        subscription_id ,\\n        region ,\\n        num_resources_by_forwarder  ,\\n        forwarder_metrics  ,\\n    )  :\\n        num_forwarders = len(num_resources_by_forwarder.keys())\\n        if num_forwarders >= MAX_FORWARDERS_PER_REGION:\\n            self.log.warning(\\n                \"Reached maximum number of forwarders (%s) in region %s, preventing scale up\",\\n                MAX_FORWARDERS_PER_REGION,\\n                region,\\n            )\\n            return False\\n\\n        def _has_enough_resources_to_scale_up(config_id )  :\\n            if num_resources_by_forwarder[config_id] < 2:\\n                self.log.warning(\"Forwarder %s only has one resource but is overwhelmed\", config_id)\\n                return False\\n            return True\\n\\n        forwarders_to_scale_up = [\\n            config_id\\n            for config_id, metrics in forwarder_metrics.items()\\n            if is_consistently_over_threshold(metrics, SCALE_UP_EXECUTION_SECONDS, self.scaling_percentage)\\n            and _has_enough_resources_to_scale_up(config_id)\\n        ]\\n\\n        if not forwarders_to_scale_up:\\n            return False\\n        region_config = self.assignment_cache[subscription_id][region]\\n\\n        # create a second forwarder for each forwarder that needs to scale up\\n        new_forwarders = await gather(*[self.create_log_forwarder(client, region) for _ in forwarders_to_scale_up])\\n        if any(new_forwarders):\\n            self.log.info(\"Scaled up %s forwarders in region %s\", len(new_forwarders), region)\\n            region_config[\"on_cooldown\"] = True\\n\\n        for overwhelmed_forwarder_id, new_forwarder in zip(forwarders_to_scale_up, new_forwarders, strict=False):\\n            if not new_forwarder:\\n                self.log.warning(\\n                    \"Failed to create new log forwarder, skipping scaling for %s\", overwhelmed_forwarder_id\\n                )\\n                continue\\n            self.split_forwarder_resources(\\n                region_config, overwhelmed_forwarder_id, new_forwarder, forwarder_metrics[overwhelmed_forwarder_id]\\n            )\\n\\n        return True\\n\\n    def split_forwarder_resources(\\n        self,\\n        region_config ,\\n        underscaled_forwarder_id ,\\n        new_forwarder ,\\n        metrics ,\\n    )  :\\n        \"\"\"Splits the resources of an underscaled forwarder between itself and a new forwarder\"\"\"\\n\\n        # add new config\\n        region_config[\"configurations\"][new_forwarder.config_id] = new_forwarder.type\\n\\n        if all(not metric[\"resource_log_volume\"] for metric in metrics):\\n            self.log.warning(\\n                \"Resource log volume metrics missing for forwarder %s, falling back to basic splitting\",\\n                underscaled_forwarder_id,\\n            )\\n            resources = sorted(\\n                resource\\n                for resource, config_id in region_config[\"resources\"].items()\\n                if config_id == underscaled_forwarder_id\\n            )\\n            split_index = len(resources) // 2\\n            region_config[\"resources\"].update(\\n                {resource: new_forwarder.config_id for resource in resources[split_index:]}\\n            )\\n            return\\n\\n        # organize resources by resource load\\n        resource_loads = {\\n            resource_id: sum(map(lambda m: m[\"resource_log_volume\"].get(resource_id, 0), metrics))\\n            for resource_id, config_id in region_config[\"resources\"].items()\\n            if config_id == underscaled_forwarder_id\\n        }\\n\\n        # reassign some resources to the new forwarder\\n        region_config[\"resources\"].update(\\n            {resource_id: new_forwarder.config_id for resource_id in resources_to_move_by_load(resource_loads)}\\n        )\\n\\n    async def scale_down_forwarders(\\n        self,\\n        client ,\\n        region ,\\n        region_config ,\\n        num_resources_by_forwarder  ,\\n        scaling_forwarder_metrics  ,\\n        all_forwarder_metrics  ,\\n    )  :\\n        \"\"\"\\n        Implements a two phased approach to scaling down forwarders:\\n        1. Move resources from the forwarder pairs which are overscaled onto just one of them\\n        2. Delete forwarders which have no resources and are not submitting logs\\n\\n        These phases will not both happen to the same forwarder in the same run,\\n        as the requirements are mutually exclusive.\\n\\n        This is required to ensure we don\\'\"'\"'t delete forwarders that are still receiving logs,\\n        as doing so would result in log loss.\\n        \"\"\"\\n\\n        # Phase 1: Move resources from the forwarder pairs which are overscaled onto just one of them\\n        forwarders_to_collapse = sorted(\\n            [\\n                config_id\\n                for config_id, metrics in scaling_forwarder_metrics.items()\\n                if is_consistently_under_threshold(metrics, SCALE_DOWN_EXECUTION_SECONDS, self.scaling_percentage)\\n                and num_resources_by_forwarder[config_id] > 0\\n            ]\\n        )\\n        # Phase 2: Delete forwarders with no resources and no metrics\\n        forwarders_to_delete = [\\n            config_id\\n            for config_id, num_resources in num_resources_by_forwarder.items()\\n            # delete forwarders with no resources and no metrics\\n            if num_resources == 0\\n            and (\\n                not all_forwarder_metrics.get(config_id)\\n                or all(not m[\"resource_log_volume\"] for m in all_forwarder_metrics[config_id])\\n            )\\n        ]\\n\\n        maybe_errors = await gather(\\n            *(\\n                self.collapse_forwarders(region_config, config_1, config_2)\\n                for config_1, config_2 in chunks(forwarders_to_collapse, 2)\\n            ),\\n            *(self.delete_log_forwarder(client, region_config, config_id) for config_id in forwarders_to_delete),\\n            return_exceptions=True,\\n        )\\n        log_errors(self.log, \"Errors during scaling down\", *maybe_errors)\\n        if forwarders_to_delete:\\n            self.log.info(\"Scaled down %s forwarders in region %s\", len(forwarders_to_delete), region)\\n\\n    async def collapse_forwarders(\\n        self, region_config , config_1 , config_2     )  :\\n        \"\"\"Collapses two forwarders into one, moving resources from config_2 to config_1. Deletion of the forwarder will happen once it is empty\"\"\"\\n        resources_to_move = {\\n            resource_id: config_1\\n            for resource_id, config_id in region_config[\"resources\"].items()\\n            if config_id == config_2\\n        }\\n        region_config[\"resources\"].update(resources_to_move)\\n\\n    async def delete_log_forwarder(\\n        self, client , region_config , config_id     )  :\\n        \"\"\"Deletes a forwarder and removes it from the configurations\"\"\"\\n        await client.delete_log_forwarder(config_id)\\n        region_config[\"configurations\"].pop(config_id, None)\\n\\n    async def clean_up_orphaned_forwarders(self, client , subscription_id )  :\\n        existing_log_forwarders = await client.list_log_forwarder_ids()\\n        orphaned_forwarders = set(existing_log_forwarders) - set(\\n            chain.from_iterable(\\n                region_config[\"configurations\"]\\n                for region_config in self.assignment_cache.get(subscription_id, {}).values()\\n            )\\n        )\\n        if not orphaned_forwarders:\\n            return\\n\\n        self.log.info(\"Cleaning up orphaned forwarders for subscription %s: %s\", subscription_id, orphaned_forwarders)\\n        await gather(\\n            *(\\n                # only try once and don\\'\"'\"'t error, if something transiently fails\\n                # we can wait til next time, we don\\'\"'\"'t want to spend much time here\\n                client.delete_log_forwarder(forwarder_id, raise_error=False, max_attempts=1)\\n                for forwarder_id in orphaned_forwarders\\n            )\\n        )\\n\\n    async def write_caches(self)  :\\n        if self.assignment_cache == self._assignment_cache_initial_state:\\n            return\\n        await write_cache(ASSIGNMENT_CACHE_BLOB, dumps(self.assignment_cache))\\n        self.log.info(\"Updated assignments stored in the cache\")\\n\\n\\nasync def main()  :\\n    await task_main(ScalingTask, [RESOURCE_CACHE_BLOB, ASSIGNMENT_CACHE_BLOB])\\n\\n\\nif __name__ == \"__main__\":  # pragma: no cover\\n    run(main())\\n'\"'\"')\n    __stickytape_write_module('\"'\"'cache/metric_blob_cache.py'\"'\"', b'\"'\"'# stdlib\\nfrom typing_extensions import Any, TypedDict\\n\\n# 3p\\nfrom cache.common import deserialize_cache\\n\\n\\nclass MetricBlobEntry(TypedDict, total=True):\\n    \"\"\"\\n    A representation of the metric blob entries\\n    \"\"\"\\n\\n    #timestamp: float\\n    \"a UNIX timestamp when metric was created\"\\n    #runtime_seconds: float\\n    \"The number of seconds taken for the forwarder to run\"\\n    #resource_log_volume: dict[str, int]\\n    \"A mapping of resource id ->log volume in number of logs\"\\n    #resource_log_bytes: dict[str, int]\\n    \"A mapping of resource id ->log volume in number of bytes\"\\n\\n\\nMETRIC_NAMES = (\"resource_log_volume\", \"resource_log_bytes\", \"runtime_seconds\")\\n\\nMETRIC_BLOB_SCHEMA   = {\\n    \"type\": \"object\",\\n    \"properties\": {\\n        \"timestamp\": {\"type\": \"number\"},\\n        \"runtime_seconds\": {\"type\": \"number\"},\\n        \"resource_log_volume\": {\"type\": \"object\", \"additionalProperties\": {\"type\": \"number\"}},\\n        \"resource_log_bytes\": {\"type\": \"object\", \"additionalProperties\": {\"type\": \"number\"}},\\n    },\\n    \"required\": [\"timestamp\", \"runtime_seconds\", \"resource_log_volume\", \"resource_log_bytes\"],\\n    \"additionalProperties\": False,\\n}\\n\\n\\ndef deserialize_blob_metric_entry(raw_metric_entry , oldest_legal_time )    :\\n    def ensure_valid_timestamp(blob_dict )    :\\n        # This is validated previously via the schema so this will always be legal\\n        return None if blob_dict[\"timestamp\"] < oldest_legal_time else blob_dict\\n\\n    return deserialize_cache(raw_metric_entry, METRIC_BLOB_SCHEMA, ensure_valid_timestamp)\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/client/log_forwarder_client.py'\"'\"', b'\"'\"'# stdlib\\nfrom asyncio import Lock, Task as AsyncTask, create_task, gather, wait\\nfrom collections.abc import Awaitable, Callable, Coroutine, Iterable\\nfrom contextlib import AbstractAsyncContextManager, suppress\\nfrom datetime import datetime, timedelta, timezone\\nfrom logging import Logger\\nfrom types import TracebackType\\nfrom typing_extensions import Any, Literal, Self, TypeAlias, TypeVar, cast\\n\\n# 3p\\nfrom aiosonic.exceptions import RequestTimeout\\nfrom azure.core.exceptions import HttpResponseError, ResourceNotFoundError, ServiceResponseTimeoutError\\nfrom azure.core.polling import AsyncLROPoller\\nfrom azure.identity.aio import DefaultAzureCredential\\nfrom azure.mgmt.appcontainers.aio import ContainerAppsAPIClient\\nfrom azure.mgmt.appcontainers.models import (\\n    Container,\\n    ContainerResources,\\n    EnvironmentVar,\\n    Job,\\n    JobConfiguration,\\n    JobConfigurationScheduleTriggerConfig,\\n    JobTemplate,\\n    ManagedEnvironment,\\n    Secret,\\n)\\nfrom azure.mgmt.storage.v2023_05_01.aio import StorageManagementClient\\nfrom azure.mgmt.storage.v2023_05_01.models import (\\n    BlobContainer,\\n    DateAfterCreation,\\n    DateAfterModification,\\n    ManagementPolicy,\\n    ManagementPolicyAction,\\n    ManagementPolicyBaseBlob,\\n    ManagementPolicyDefinition,\\n    ManagementPolicyFilter,\\n    ManagementPolicyName,\\n    ManagementPolicyRule,\\n    ManagementPolicySchema,\\n    ManagementPolicySnapShot,\\n    PublicNetworkAccess,\\n    Sku,\\n    StorageAccount,\\n    StorageAccountCreateParameters,\\n    StorageAccountKey,\\n)\\nfrom azure.storage.blob.aio import ContainerClient\\nfrom azure.storage.blob.aio._download_async import StorageStreamDownloader\\nfrom datadog_api_client import AsyncApiClient, Configuration\\nfrom datadog_api_client.v2.api.metrics_api import MetricsApi\\nfrom datadog_api_client.v2.model.intake_payload_accepted import IntakePayloadAccepted\\nfrom datadog_api_client.v2.model.metric_intake_type import MetricIntakeType\\nfrom datadog_api_client.v2.model.metric_payload import MetricPayload\\nfrom datadog_api_client.v2.model.metric_point import MetricPoint\\nfrom datadog_api_client.v2.model.metric_resource import MetricResource\\nfrom datadog_api_client.v2.model.metric_series import MetricSeries\\nfrom tenacity import RetryCallState, RetryError, retry, stop_after_attempt\\n\\n# project\\nfrom cache.common import (\\n    STORAGE_ACCOUNT_TYPE,\\n    LogForwarderType,\\n)\\nfrom cache.env import (\\n    CONFIG_ID_SETTING,\\n    CONNECTION_STRING_SECRET,\\n    CONTROL_PLANE_ID_SETTING,\\n    CONTROL_PLANE_REGION_SETTING,\\n    DD_API_KEY_SECRET,\\n    DD_API_KEY_SETTING,\\n    DD_SITE_SETTING,\\n    DD_TELEMETRY_SETTING,\\n    FORWARDER_IMAGE_SETTING,\\n    STORAGE_CONNECTION_SETTING,\\n    get_config_option,\\n    is_truthy,\\n)\\nfrom cache.metric_blob_cache import METRIC_NAMES, MetricBlobEntry, deserialize_blob_metric_entry\\nfrom tasks.common import (\\n    FORWARDER_CONTAINER_APP_PREFIX,\\n    FORWARDER_STORAGE_ACCOUNT_PREFIX,\\n    Resource,\\n    get_container_app_name,\\n    get_managed_env_id,\\n    get_managed_env_name,\\n    get_storage_account_name,\\n    log_errors,\\n)\\nfrom tasks.concurrency import collect, create_task_from_awaitable\\nfrom tasks.constants import ALLOWED_CONTAINER_APP_REGIONS\\nfrom tasks.deploy_common import wait_for_resource\\n\\nFORWARDER_METRIC_CONTAINER_NAME = \"dd-forwarder\"\\n\\nINTERNAL_METRIC_PREFIX = \"azure.log_forwarder.\"\\n\\nFORWARDER_TIMEOUT_SECONDS = 1800  # 30 minutes\\nCLIENT_MAX_SECONDS = 5\\nMAX_ATTEMPS = 5\\n\\nFORWARDER_METRIC_BLOB_LIFETIME_DAYS = 1\\n\\n\\nT = TypeVar(\"T\")\\n\\n\\nasync def ignore_exception_type(exc , a )    :\\n    try:\\n        return await a\\n    except exc:\\n        return None\\n\\n\\nasync def is_exception_retryable(state )  :\\n    if (future := state.outcome) and (e := future.exception()):\\n        if isinstance(e, HttpResponseError):\\n            return e.status_code is not None and (e.status_code == 429 or e.status_code >= 500)\\n        if isinstance(e, RequestTimeout | ServiceResponseTimeoutError):\\n            return True\\n    return False\\n\\n\\nResourcePoller  = tuple[AsyncLROPoller[T], Callable[[], Awaitable[T]]]\\n\\n\\ndef get_datetime_str(time )  :\\n    return f\"{time:%Y-%m-%d-%H}\"\\n\\n\\ndef get_metric_value(\\n    metric_entry , metric_name   )  :\\n    value = metric_entry[metric_name]\\n    if isinstance(value, dict):\\n        return sum(value.values())\\n    return value\\n\\n\\nclass LogForwarderClient(AbstractAsyncContextManager[\"LogForwarderClient\"]):\\n    def __init__(\\n        self, log , credential , subscription_id , resource_group     )  :\\n        self.forwarder_image = get_config_option(FORWARDER_IMAGE_SETTING)\\n        self.dd_api_key = get_config_option(DD_API_KEY_SETTING)\\n        self.dd_site = get_config_option(DD_SITE_SETTING)\\n        self.control_plane_region = get_config_option(CONTROL_PLANE_REGION_SETTING)\\n        self.control_plane_id = get_config_option(CONTROL_PLANE_ID_SETTING)\\n        self.should_submit_metrics = is_truthy(DD_TELEMETRY_SETTING)\\n        self.log = log\\n        self.resource_group = resource_group\\n        self.subscription_id = subscription_id\\n        self.container_apps_client = ContainerAppsAPIClient(credential, subscription_id)\\n        self.storage_client = StorageManagementClient(credential, subscription_id)\\n        self._datadog_client = AsyncApiClient(Configuration(request_timeout=CLIENT_MAX_SECONDS))\\n        self.metrics_client = MetricsApi(self._datadog_client)\\n        self._blob_forwarder_data_lock = Lock()\\n        self._blob_forwarder_data    = None\\n        self._background_tasks  = set()\\n        self.log_extra = {\"subscription_id\": self.subscription_id, \"resource_group\": self.resource_group}\\n\\n    async def __aenter__(self)  :\\n        await gather(\\n            self.container_apps_client.__aenter__(),\\n            self.storage_client.__aenter__(),\\n            self._datadog_client.__aenter__(),\\n        )\\n        return self\\n\\n    async def __aexit__(\\n        self, exc_type   , exc_val   , exc_tb       )  :\\n        if self._background_tasks:\\n            await wait(self._background_tasks)\\n        await gather(\\n            self.container_apps_client.__aexit__(exc_type, exc_val, exc_tb),\\n            self.storage_client.__aexit__(exc_type, exc_val, exc_tb),\\n            self._datadog_client.__aexit__(exc_type, exc_val, exc_tb),\\n        )\\n\\n    def submit_background_task(self, coro   )  :\\n        def _done_callback(task )  :\\n            self._background_tasks.discard(task)\\n            if e := task.exception():\\n                self.log.error(\"Background task failed with an exception\", exc_info=e, extra=self.log_extra)\\n\\n        task = create_task(coro)\\n        self._background_tasks.add(task)\\n        task.add_done_callback(_done_callback)\\n\\n    def get_container_app_region(self, region )  :\\n        if region in ALLOWED_CONTAINER_APP_REGIONS:\\n            return region\\n        return self.control_plane_region\\n\\n    async def create_log_forwarder(self, region , config_id )  :\\n        storage_account_name = get_storage_account_name(config_id)\\n\\n        await wait_for_resource(*await self.create_log_forwarder_storage_account(region, storage_account_name))\\n\\n        maybe_errors = await gather(\\n            wait_for_resource(*await self.create_or_update_log_forwarder_container_app(region, config_id)),\\n            self.create_log_forwarder_containers(storage_account_name),\\n            self.create_log_forwarder_storage_management_policy(storage_account_name),\\n            return_exceptions=True,\\n        )\\n        log_errors(\\n            self.log,\\n            \"Failed to create function app and/or get blob forwarder data\",\\n            *maybe_errors,\\n            reraise=True,\\n            extra=self.log_extra,\\n        )\\n\\n        # for now this is the only type we support\\n        return STORAGE_ACCOUNT_TYPE\\n\\n    async def get_forwarder_resources(self, config_id )       :\\n        # spawn them off at the same time\\n        get_job = create_task_from_awaitable(\\n            self.container_apps_client.jobs.get(self.resource_group, get_container_app_name(config_id))\\n        )\\n        get_secrets = create_task_from_awaitable(\\n            self.container_apps_client.jobs.list_secrets(self.resource_group, get_container_app_name(config_id))\\n        )\\n        get_storage_account = create_task_from_awaitable(\\n            self.storage_client.storage_accounts.get_properties(\\n                self.resource_group, get_storage_account_name(config_id)\\n            )\\n        )\\n\\n        job = None\\n        with suppress(ResourceNotFoundError):\\n            job = await get_job\\n            # populate the configuration separately\\n            if not job.configuration:\\n                job.configuration = JobConfiguration(replica_timeout=FORWARDER_TIMEOUT_SECONDS)\\n            job.configuration.secrets = (await get_secrets).value\\n        storage_account = None\\n        with suppress(ResourceNotFoundError):\\n            storage_account = await get_storage_account\\n        return job, storage_account\\n\\n    async def create_log_forwarder_storage_account(\\n        self, region , storage_account_name     )  :\\n        self.log.info(\"Creating storage account %s for region %s\", storage_account_name, region, extra=self.log_extra)\\n        return await self.storage_client.storage_accounts.begin_create(\\n            self.resource_group,\\n            storage_account_name,\\n            StorageAccountCreateParameters(\\n                sku=Sku(\\n                    # TODO (AZINTS-2646): figure out which SKU we should be using here\\n                    name=\"Standard_LRS\"\\n                ),\\n                kind=\"StorageV2\",\\n                location=region,\\n                public_network_access=PublicNetworkAccess.ENABLED,\\n            ),\\n        ), lambda: self.storage_client.storage_accounts.get_properties(self.resource_group, storage_account_name)\\n\\n    async def create_log_forwarder_managed_environment(self, region , wait  = False)  :\\n        container_app_region = self.get_container_app_region(region)\\n        env_name = get_managed_env_name(container_app_region, self.control_plane_id)\\n        self.log.info(\\n            \"Creating managed environment %s for region %s in %s\",\\n            env_name,\\n            region,\\n            container_app_region,\\n            extra=self.log_extra,\\n        )\\n        poller = await self.container_apps_client.managed_environments.begin_create_or_update(\\n            self.resource_group,\\n            env_name,\\n            ManagedEnvironment(\\n                location=container_app_region,\\n                zone_redundant=False,\\n            ),\\n        )\\n        if wait:\\n            await poller.result()\\n\\n    async def get_log_forwarder_managed_environment(self, region )    :\\n        env_name = get_managed_env_name(self.get_container_app_region(region), self.control_plane_id)\\n        try:\\n            managed_env = await self.container_apps_client.managed_environments.get(self.resource_group, env_name)\\n        except ResourceNotFoundError:\\n            return None\\n        return str(managed_env.id)\\n\\n    async def create_or_update_log_forwarder_container_app(\\n        self,\\n        region ,\\n        config_id ,\\n        *,\\n        env    = None,\\n        secrets    = None,\\n    )  :\\n        job_name = get_container_app_name(config_id)\\n        region = self.get_container_app_region(region)\\n        env = env or self.generate_forwarder_settings(config_id)\\n        secrets = secrets or await self.generate_forwarder_secrets(config_id)\\n        return await self.container_apps_client.jobs.begin_create_or_update(\\n            self.resource_group,\\n            job_name,\\n            Job(\\n                location=region,\\n                environment_id=get_managed_env_id(\\n                    self.subscription_id, self.resource_group, region, self.control_plane_id\\n                ),\\n                configuration=JobConfiguration(\\n                    trigger_type=\"Schedule\",\\n                    schedule_trigger_config=JobConfigurationScheduleTriggerConfig(\\n                        cron_expression=\"* * * * *\",\\n                        parallelism=1,\\n                        replica_completion_count=1,\\n                    ),\\n                    replica_timeout=FORWARDER_TIMEOUT_SECONDS,\\n                    replica_retry_limit=1,\\n                    secrets=secrets,\\n                ),\\n                template=JobTemplate(\\n                    containers=[\\n                        Container(\\n                            name=\"forwarder\",\\n                            image=self.forwarder_image,\\n                            resources=ContainerResources(cpu=2, memory=\"4Gi\"),\\n                            env=env,\\n                        )\\n                    ],\\n                ),\\n            ),\\n        ), lambda: self.container_apps_client.jobs.get(self.resource_group, job_name)\\n\\n    async def generate_forwarder_secrets(self, config_id )  :\\n        connection_string = await self.get_connection_string(get_storage_account_name(config_id))\\n        return [\\n            Secret(name=DD_API_KEY_SECRET, value=self.dd_api_key),\\n            Secret(name=CONNECTION_STRING_SECRET, value=connection_string),\\n        ]\\n\\n    def generate_forwarder_settings(self, config_id )  :\\n        return [\\n            EnvironmentVar(name=STORAGE_CONNECTION_SETTING, secret_ref=CONNECTION_STRING_SECRET),\\n            EnvironmentVar(name=DD_API_KEY_SETTING, secret_ref=DD_API_KEY_SECRET),\\n            EnvironmentVar(name=DD_SITE_SETTING, value=self.dd_site),\\n            EnvironmentVar(name=CONTROL_PLANE_ID_SETTING, value=self.control_plane_id),\\n            EnvironmentVar(name=CONFIG_ID_SETTING, value=config_id),\\n        ]\\n\\n    async def create_log_forwarder_containers(self, storage_account_name )  :\\n        await self.storage_client.blob_containers.create(\\n            self.resource_group,\\n            storage_account_name,\\n            FORWARDER_METRIC_CONTAINER_NAME,\\n            BlobContainer(),\\n        )\\n\\n    async def create_log_forwarder_storage_management_policy(self, storage_account_name )  :\\n        await self.storage_client.management_policies.create_or_update(\\n            self.resource_group,\\n            storage_account_name,\\n            ManagementPolicyName.DEFAULT,\\n            ManagementPolicy(\\n                policy=ManagementPolicySchema(\\n                    rules=[\\n                        ManagementPolicyRule(\\n                            enabled=True,\\n                            name=\"Delete Old Metric Blobs\",\\n                            type=\"Lifecycle\",\\n                            definition=ManagementPolicyDefinition(\\n                                actions=ManagementPolicyAction(\\n                                    base_blob=ManagementPolicyBaseBlob(\\n                                        delete=DateAfterModification(\\n                                            days_after_modification_greater_than=FORWARDER_METRIC_BLOB_LIFETIME_DAYS\\n                                        )\\n                                    ),\\n                                    snapshot=ManagementPolicySnapShot(\\n                                        delete=DateAfterCreation(\\n                                            days_after_creation_greater_than=FORWARDER_METRIC_BLOB_LIFETIME_DAYS\\n                                        )\\n                                    ),\\n                                ),\\n                                filters=ManagementPolicyFilter(\\n                                    blob_types=[\"blockBlob\", \"appendBlob\"],\\n                                ),\\n                            ),\\n                        )\\n                    ]\\n                )\\n            ),\\n        )\\n\\n    async def get_connection_string(self, storage_account_name )  :\\n        keys_result = await self.storage_client.storage_accounts.list_keys(self.resource_group, storage_account_name)\\n        keys  = keys_result.keys  # type: ignore\\n        if len(keys) == 0:\\n            raise ValueError(\"No keys found for storage account\")\\n        key  = keys[0].value  # type: ignore\\n        return (\\n            \"DefaultEndpointsProtocol=https;AccountName=\"\\n            + storage_account_name\\n            + \";AccountKey=\"\\n            + key\\n            + \";EndpointSuffix=core.windows.net\"\\n        )\\n\\n    async def delete_log_forwarder(self, forwarder_id , *, raise_error  = True, max_attempts  = 3)  :\\n        \"\"\"Deletes the Log forwarder, returns True if successful, False otherwise\"\"\"\\n\\n        @retry(stop=stop_after_attempt(max_attempts), retry=is_exception_retryable)\\n        async def _delete_forwarder()  :\\n            self.log.info(\"Attempting to delete log forwarder %s\", forwarder_id, extra=self.log_extra)\\n\\n            # start deleting the storage account now, it has no dependencies\\n            delete_storage_account_task = create_task(\\n                ignore_exception_type(\\n                    ResourceNotFoundError,\\n                    self.storage_client.storage_accounts.delete(\\n                        self.resource_group, get_storage_account_name(forwarder_id)\\n                    ),\\n                )\\n            )\\n\\n            poller = await ignore_exception_type(\\n                ResourceNotFoundError,\\n                self.container_apps_client.jobs.begin_delete(self.resource_group, get_container_app_name(forwarder_id)),\\n            )\\n            if poller:\\n                await poller.result()\\n\\n                await delete_storage_account_task\\n            self.log.info(\"Deleted log forwarder %s\", forwarder_id, extra=self.log_extra)\\n\\n        try:\\n            await _delete_forwarder()\\n            return True\\n        except Exception:\\n            if raise_error:\\n                raise\\n            return False\\n\\n    async def delete_log_forwarder_env(self, region , *, raise_error  = True, max_attempts  = 3)  :\\n        \"\"\"Deletes the Log forwarder env, returns True if successful, False otherwise\"\"\"\\n\\n        @retry(stop=stop_after_attempt(max_attempts), retry=is_exception_retryable)\\n        async def _delete_forwarder_env()  :\\n            self.log.info(\\n                \"Attempting to delete log forwarder env for region %s and control plane %s\",\\n                region,\\n                self.control_plane_id,\\n                extra=self.log_extra,\\n            )\\n\\n            poller = await ignore_exception_type(\\n                ResourceNotFoundError,\\n                self.container_apps_client.managed_environments.begin_delete(\\n                    self.resource_group, get_managed_env_name(region, self.control_plane_id)\\n                ),\\n            )\\n            if poller:\\n                await poller.result()\\n\\n            self.log.info(\\n                \"Deleted log forwarder env for region %s and control plane %s\",\\n                region,\\n                self.control_plane_id,\\n                extra=self.log_extra,\\n            )\\n\\n        try:\\n            await _delete_forwarder_env()\\n            return True\\n        except Exception:\\n            if raise_error:\\n                raise\\n            return False\\n\\n    async def collect_forwarder_metrics(self, config_id , oldest_valid_timestamp )  :\\n        \"\"\"Collects metrics for a given forwarder and submits them to the metrics endpoint\"\"\"\\n        metric_lines = await self.get_blob_metrics_lines(config_id)\\n        forwarder_metrics = [\\n            metric_entry\\n            for metric_line in metric_lines\\n            if (metric_entry := deserialize_blob_metric_entry(metric_line, oldest_valid_timestamp))\\n        ]\\n        if not forwarder_metrics:\\n            self.log.warning(\"No valid metrics found for forwarder %s\", config_id, extra=self.log_extra)\\n        self.submit_background_task(self.submit_log_forwarder_metrics(config_id, forwarder_metrics))\\n        return forwarder_metrics\\n\\n    async def get_blob_metrics_lines(self, config_id )  :\\n        \"\"\"\\n        Returns a list of json decodable strings that represent metrics\\n        json string takes form of {\\'\"'\"'Values\\'\"'\"': [metric_dict]}\\n        metric_dict is as follows {\\'\"'\"'Name\\'\"'\"': str, \\'\"'\"'Value\\'\"'\"': float, \\'\"'\"'Time\\'\"'\"': float}\\n        Time is a unix timestamp\\n        \"\"\"\\n        conn_str = await self.get_connection_string(get_storage_account_name(config_id))\\n        async with ContainerClient.from_connection_string(\\n            conn_str, FORWARDER_METRIC_CONTAINER_NAME\\n        ) as container_client:\\n            current_time  = datetime.now(timezone.utc)\\n            previous_hour  = current_time - timedelta(hours=1)\\n            current_blob_name = f\"metrics_{get_datetime_str(current_time)}.json\"\\n            previous_blob_name = f\"metrics_{get_datetime_str(previous_hour)}.json\"\\n            results = await gather(\\n                *[\\n                    self.read_blob(container_client, previous_blob_name),\\n                    self.read_blob(container_client, current_blob_name),\\n                ],\\n                return_exceptions=True,\\n            )\\n            metric_lines  = []\\n            for result, blob in zip(results, [previous_blob_name, current_blob_name], strict=False):\\n                if isinstance(result, str):\\n                    metric_lines.extend(result.splitlines())\\n                else:\\n                    msg = \"\"\\n                    if isinstance(result, RetryError):\\n                        msg = \"Max retries attempted, failed due to:\\\\n\"\\n                        result = result.last_attempt.exception() or \"Unknown\"\\n                    if isinstance(result, HttpResponseError):\\n                        msg += f\"HttpResponseError with Response Code: {result.status_code}\\\\nError: {result.error or result.reason or result.message}\"\\n                    else:\\n                        msg += str(result)\\n                    self.log.error(\\n                        \"Unable to fetch metrics in %s for forwarder %s:\\\\n%s\",\\n                        blob,\\n                        config_id,\\n                        msg,\\n                        extra=self.log_extra,\\n                    )\\n\\n            return metric_lines\\n\\n    @retry(retry=is_exception_retryable, stop=stop_after_attempt(MAX_ATTEMPS))\\n    async def read_blob(self, container_client , blob_name )  :\\n        try:\\n            async with container_client.get_blob_client(blob_name) as blob_client:\\n                raw_data  = await blob_client.download_blob(timeout=CLIENT_MAX_SECONDS)\\n                dict_str = await raw_data.readall()\\n                return dict_str.decode(\"utf-8\")\\n        except ResourceNotFoundError:\\n            return \"\"\\n\\n    @retry(retry=is_exception_retryable, stop=stop_after_attempt(MAX_ATTEMPS))\\n    async def submit_log_forwarder_metrics(self, log_forwarder_id , metrics )  :\\n        if not self.should_submit_metrics or not metrics:\\n            return\\n\\n        response  = await self.metrics_client.submit_metrics(\\n            body=self.create_metric_payload(metrics, log_forwarder_id)\\n        )  # type: ignore\\n        for error in response.get(\"errors\", []):\\n            self.log.error(error, extra=self.log_extra)\\n\\n    def create_metric_payload(self, metric_entries , log_forwarder_id )  :\\n        # type ignore hack to get pyright typing to work since the SDK overrides __new__\\n        log_forwarder_name = get_container_app_name(log_forwarder_id)\\n        return cast(\\n            MetricPayload,\\n            MetricPayload(\\n                series=[\\n                    MetricSeries(\\n                        metric=INTERNAL_METRIC_PREFIX + metric_name,\\n                        type=MetricIntakeType.UNSPECIFIED,\\n                        points=[\\n                            MetricPoint(\\n                                timestamp=int(metric_entry[\"timestamp\"]),\\n                                value=get_metric_value(metric_entry, metric_name),\\n                            )\\n                            for metric_entry in metric_entries\\n                        ],\\n                        resources=[\\n                            MetricResource(\\n                                name=log_forwarder_name,\\n                                type=\"logforwarder\",\\n                            ),\\n                        ],\\n                    )\\n                    for metric_name in METRIC_NAMES\\n                ]\\n            ),\\n        )\\n\\n    async def list_log_forwarder_ids(self)  :\\n        jobs, storage_accounts = await gather(\\n            collect(self.container_apps_client.jobs.list_by_resource_group(self.resource_group)),\\n            collect(self.storage_client.storage_accounts.list_by_resource_group(self.resource_group)),\\n        )\\n\\n        def _get_forwarder_config_ids(it , prefix )  :\\n            return {resource.name.removeprefix(prefix) for resource in it if resource.name.startswith(prefix)}\\n\\n        return _get_forwarder_config_ids(\\n            cast(Iterable[Resource], jobs), FORWARDER_CONTAINER_APP_PREFIX\\n        ) | _get_forwarder_config_ids(cast(Iterable[Resource], storage_accounts), FORWARDER_STORAGE_ACCOUNT_PREFIX)\\n'\"'\"')\n    __stickytape_write_module('\"'\"'tasks/deploy_common.py'\"'\"', b'\"'\"'# stdlib\\nfrom collections.abc import Awaitable, Callable\\nfrom typing_extensions import Any, TypeVar\\n\\n# 3p\\nfrom azure.core.exceptions import ResourceNotFoundError\\nfrom azure.core.polling import AsyncLROPoller\\nfrom tenacity import retry, retry_if_exception_type, stop_after_delay\\n\\nT = TypeVar(\"T\")\\n\\n\\nasync def wait_for_resource(\\n    poller , confirm  , wait_seconds  = 30\\n)  :\\n    \"\"\"Wait for the poller to complete and confirm the resource exists,\\n    if the resource does not exist, `confirm` should throw a ResourceNotFoundError\"\"\"\\n    res = await poller.result()\\n\\n    await retry(\\n        retry=retry_if_exception_type(ResourceNotFoundError),\\n        stop=stop_after_delay(wait_seconds),\\n    )(confirm)()\\n\\n    return res\\n'\"'\"')\n    from tasks.diagnostic_settings_task import DIAGNOSTIC_SETTINGS_TASK_NAME\n    from tasks.resources_task import RESOURCES_TASK_NAME\n    from tasks.scaling_task import SCALING_TASK_NAME\n    \n    for task in (RESOURCES_TASK_NAME, DIAGNOSTIC_SETTINGS_TASK_NAME, SCALING_TASK_NAME):\n        print(f\"hello {task}\")\n    # Start-AzContainerAppJob -Name ${deployerTaskName} -ResourceGroupName ${controlPlaneResourceGroupName}\n    '\n",
            "deployerTaskImage": "[format('{0}/deployer:latest', parameters('imageRegistry'))]",
            "forwarderImage": "[format('{0}/forwarder:latest', parameters('imageRegistry'))]",
            "STORAGE_CONNECTION_SETTING": "AzureWebJobsStorage",
            "DD_SITE_SETTING": "DD_SITE",
            "DD_API_KEY_SETTING": "DD_API_KEY",
            "DD_APP_KEY_SETTING": "DD_APP_KEY",
            "DD_TELEMETRY_SETTING": "DD_TELEMETRY",
            "FORWARDER_IMAGE_SETTING": "FORWARDER_IMAGE",
            "SUBSCRIPTION_ID_SETTING": "SUBSCRIPTION_ID",
            "RESOURCE_GROUP_SETTING": "RESOURCE_GROUP",
            "CONTROL_PLANE_REGION_SETTING": "CONTROL_PLANE_REGION",
            "CONTROL_PLANE_ID_SETTING": "CONTROL_PLANE_ID",
            "MONITORED_SUBSCRIPTIONS_SETTING": "MONITORED_SUBSCRIPTIONS",
            "STORAGE_ACCOUNT_URL_SETTING": "STORAGE_ACCOUNT_URL",
            "LOG_LEVEL_SETTING": "LOG_LEVEL",
            "DD_API_KEY_SECRET": "dd-api-key",
            "DD_APP_KEY_SECRET": "dd-app-key",
            "CONNECTION_STRING_SECRET": "connection-string",
            "resourceTaskName": "[format('resources-task-{0}', parameters('controlPlaneId'))]",
            "diagnosticSettingsTaskName": "[format('diagnostic-settings-task-{0}', parameters('controlPlaneId'))]",
            "scalingTaskName": "[format('scaling-task-{0}', parameters('controlPlaneId'))]",
            "deployerTaskName": "[format('deployer-task-{0}', parameters('controlPlaneId'))]",
            "containerAppStartRoleName": "[format('ContainerAppStartRole{0}', parameters('controlPlaneId'))]"
          },
          "resources": [
            {
              "type": "Microsoft.Web/serverfarms",
              "apiVersion": "2022-09-01",
              "name": "[format('control-plane-asp-{0}', parameters('controlPlaneId'))]",
              "location": "[parameters('controlPlaneLocation')]",
              "kind": "linux",
              "properties": {
                "reserved": true
              },
              "sku": {
                "tier": "Dynamic",
                "name": "Y1"
              }
            },
            {
              "type": "Microsoft.Storage/storageAccounts",
              "apiVersion": "2023-01-01",
              "name": "[format('lfostorage{0}', parameters('controlPlaneId'))]",
              "kind": "StorageV2",
              "location": "[parameters('controlPlaneLocation')]",
              "properties": {
                "accessTier": "Hot"
              },
              "sku": {
                "name": "Standard_LRS"
              }
            },
            {
              "type": "Microsoft.Storage/storageAccounts/fileServices",
              "apiVersion": "2023-05-01",
              "name": "[format('{0}/{1}', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]",
              "properties": {},
              "dependsOn": [
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.Storage/storageAccounts/blobServices",
              "apiVersion": "2023-01-01",
              "name": "[format('{0}/{1}', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]",
              "properties": {},
              "dependsOn": [
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.Storage/storageAccounts/blobServices/containers",
              "apiVersion": "2023-01-01",
              "name": "[format('{0}/{1}/{2}', format('lfostorage{0}', parameters('controlPlaneId')), 'default', 'control-plane-cache')]",
              "properties": {},
              "dependsOn": [
                "[resourceId('Microsoft.Storage/storageAccounts/blobServices', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]"
              ]
            },
            {
              "type": "Microsoft.Web/sites",
              "apiVersion": "2022-09-01",
              "name": "[variables('resourceTaskName')]",
              "location": "[parameters('controlPlaneLocation')]",
              "kind": "functionapp",
              "identity": {
                "type": "SystemAssigned"
              },
              "properties": {
                "serverFarmId": "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "siteConfig": {
                  "appSettings": "[union(createArray(createObject('name', variables('STORAGE_CONNECTION_SETTING'), 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('DD_API_KEY_SETTING'), 'value', parameters('datadogApiKey')), createObject('name', variables('DD_SITE_SETTING'), 'value', parameters('datadogSite')), createObject('name', variables('DD_TELEMETRY_SETTING'), 'value', if(parameters('datadogTelemetry'), 'true', 'false')), createObject('name', variables('CONTROL_PLANE_ID_SETTING'), 'value', parameters('controlPlaneId')), createObject('name', 'AzureWebJobsFeatureFlags', 'value', 'EnableWorkerIndexing'), createObject('name', 'FUNCTIONS_EXTENSION_VERSION', 'value', '~4'), createObject('name', 'FUNCTIONS_WORKER_RUNTIME', 'value', 'python'), createObject('name', 'WEBSITE_CONTENTAZUREFILECONNECTIONSTRING', 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('LOG_LEVEL_SETTING'), 'value', parameters('logLevel'))), createArray(createObject('name', 'WEBSITE_CONTENTSHARE', 'value', variables('resourceTaskName')), createObject('name', variables('MONITORED_SUBSCRIPTIONS_SETTING'), 'value', parameters('monitoredSubscriptions'))))]",
                  "linuxFxVersion": "Python|3.11"
                },
                "publicNetworkAccess": "Enabled",
                "httpsOnly": true
              },
              "dependsOn": [
                "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "[resourceId('Microsoft.Storage/storageAccounts/fileServices', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]",
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.Web/sites",
              "apiVersion": "2022-09-01",
              "name": "[variables('diagnosticSettingsTaskName')]",
              "location": "[parameters('controlPlaneLocation')]",
              "kind": "functionapp",
              "identity": {
                "type": "SystemAssigned"
              },
              "properties": {
                "serverFarmId": "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "siteConfig": {
                  "appSettings": "[union(createArray(createObject('name', variables('STORAGE_CONNECTION_SETTING'), 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('DD_API_KEY_SETTING'), 'value', parameters('datadogApiKey')), createObject('name', variables('DD_SITE_SETTING'), 'value', parameters('datadogSite')), createObject('name', variables('DD_TELEMETRY_SETTING'), 'value', if(parameters('datadogTelemetry'), 'true', 'false')), createObject('name', variables('CONTROL_PLANE_ID_SETTING'), 'value', parameters('controlPlaneId')), createObject('name', 'AzureWebJobsFeatureFlags', 'value', 'EnableWorkerIndexing'), createObject('name', 'FUNCTIONS_EXTENSION_VERSION', 'value', '~4'), createObject('name', 'FUNCTIONS_WORKER_RUNTIME', 'value', 'python'), createObject('name', 'WEBSITE_CONTENTAZUREFILECONNECTIONSTRING', 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('LOG_LEVEL_SETTING'), 'value', parameters('logLevel'))), createArray(createObject('name', variables('RESOURCE_GROUP_SETTING'), 'value', parameters('controlPlaneResourceGroupName')), createObject('name', 'WEBSITE_CONTENTSHARE', 'value', variables('resourceTaskName'))))]",
                  "linuxFxVersion": "Python|3.11"
                },
                "publicNetworkAccess": "Enabled",
                "httpsOnly": true
              },
              "dependsOn": [
                "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "[resourceId('Microsoft.Storage/storageAccounts/fileServices', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]",
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.Web/sites",
              "apiVersion": "2022-09-01",
              "name": "[variables('scalingTaskName')]",
              "location": "[parameters('controlPlaneLocation')]",
              "kind": "functionapp",
              "identity": {
                "type": "SystemAssigned"
              },
              "properties": {
                "serverFarmId": "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "siteConfig": {
                  "appSettings": "[union(createArray(createObject('name', variables('STORAGE_CONNECTION_SETTING'), 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('DD_API_KEY_SETTING'), 'value', parameters('datadogApiKey')), createObject('name', variables('DD_SITE_SETTING'), 'value', parameters('datadogSite')), createObject('name', variables('DD_TELEMETRY_SETTING'), 'value', if(parameters('datadogTelemetry'), 'true', 'false')), createObject('name', variables('CONTROL_PLANE_ID_SETTING'), 'value', parameters('controlPlaneId')), createObject('name', 'AzureWebJobsFeatureFlags', 'value', 'EnableWorkerIndexing'), createObject('name', 'FUNCTIONS_EXTENSION_VERSION', 'value', '~4'), createObject('name', 'FUNCTIONS_WORKER_RUNTIME', 'value', 'python'), createObject('name', 'WEBSITE_CONTENTAZUREFILECONNECTIONSTRING', 'value', format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)), createObject('name', variables('LOG_LEVEL_SETTING'), 'value', parameters('logLevel'))), createArray(createObject('name', variables('RESOURCE_GROUP_SETTING'), 'value', parameters('controlPlaneResourceGroupName')), createObject('name', 'WEBSITE_CONTENTSHARE', 'value', variables('resourceTaskName')), createObject('name', variables('FORWARDER_IMAGE_SETTING'), 'value', variables('forwarderImage')), createObject('name', variables('DD_APP_KEY_SETTING'), 'value', parameters('datadogApplicationKey')), createObject('name', variables('CONTROL_PLANE_REGION_SETTING'), 'value', parameters('controlPlaneLocation'))))]",
                  "linuxFxVersion": "Python|3.11"
                },
                "publicNetworkAccess": "Enabled",
                "httpsOnly": true
              },
              "dependsOn": [
                "[resourceId('Microsoft.Web/serverfarms', format('control-plane-asp-{0}', parameters('controlPlaneId')))]",
                "[resourceId('Microsoft.Storage/storageAccounts/fileServices', format('lfostorage{0}', parameters('controlPlaneId')), 'default')]",
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.App/managedEnvironments",
              "apiVersion": "2024-03-01",
              "name": "[format('dd-log-forwarder-env-{0}-{1}', parameters('controlPlaneId'), parameters('controlPlaneLocation'))]",
              "location": "[parameters('controlPlaneLocation')]",
              "properties": {}
            },
            {
              "type": "Microsoft.App/jobs",
              "apiVersion": "2024-03-01",
              "name": "[variables('deployerTaskName')]",
              "location": "[parameters('controlPlaneLocation')]",
              "identity": {
                "type": "SystemAssigned"
              },
              "properties": {
                "environmentId": "[resourceId('Microsoft.App/managedEnvironments', format('dd-log-forwarder-env-{0}-{1}', parameters('controlPlaneId'), parameters('controlPlaneLocation')))]",
                "configuration": {
                  "triggerType": "Schedule",
                  "scheduleTriggerConfig": {
                    "cronExpression": "*/30 * * * *"
                  },
                  "replicaRetryLimit": 1,
                  "replicaTimeout": 1800,
                  "secrets": [
                    {
                      "name": "[variables('CONNECTION_STRING_SECRET')]",
                      "value": "[format('DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}', format('lfostorage{0}', parameters('controlPlaneId')), listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value)]"
                    },
                    {
                      "name": "[variables('DD_API_KEY_SECRET')]",
                      "value": "[parameters('datadogApiKey')]"
                    },
                    {
                      "name": "[variables('DD_APP_KEY_SECRET')]",
                      "value": "[parameters('datadogApplicationKey')]"
                    }
                  ]
                },
                "template": {
                  "containers": [
                    {
                      "name": "[variables('deployerTaskName')]",
                      "image": "[variables('deployerTaskImage')]",
                      "resources": {
                        "cpu": "[json('0.5')]",
                        "memory": "1Gi"
                      },
                      "env": [
                        {
                          "name": "[variables('STORAGE_CONNECTION_SETTING')]",
                          "secretRef": "[variables('CONNECTION_STRING_SECRET')]"
                        },
                        {
                          "name": "[variables('SUBSCRIPTION_ID_SETTING')]",
                          "value": "[parameters('controlPlaneSubscriptionId')]"
                        },
                        {
                          "name": "[variables('RESOURCE_GROUP_SETTING')]",
                          "value": "[parameters('controlPlaneResourceGroupName')]"
                        },
                        {
                          "name": "[variables('CONTROL_PLANE_ID_SETTING')]",
                          "value": "[parameters('controlPlaneId')]"
                        },
                        {
                          "name": "[variables('CONTROL_PLANE_REGION_SETTING')]",
                          "value": "[parameters('controlPlaneLocation')]"
                        },
                        {
                          "name": "[variables('DD_API_KEY_SETTING')]",
                          "secretRef": "[variables('DD_API_KEY_SECRET')]"
                        },
                        {
                          "name": "[variables('DD_APP_KEY_SETTING')]",
                          "secretRef": "[variables('DD_APP_KEY_SECRET')]"
                        },
                        {
                          "name": "[variables('DD_SITE_SETTING')]",
                          "value": "[parameters('datadogSite')]"
                        },
                        {
                          "name": "[variables('DD_TELEMETRY_SETTING')]",
                          "value": "[if(parameters('datadogTelemetry'), 'true', 'false')]"
                        },
                        {
                          "name": "[variables('STORAGE_ACCOUNT_URL_SETTING')]",
                          "value": "[parameters('storageAccountUrl')]"
                        },
                        {
                          "name": "[variables('LOG_LEVEL_SETTING')]",
                          "value": "[parameters('logLevel')]"
                        }
                      ]
                    }
                  ]
                }
              },
              "dependsOn": [
                "[resourceId('Microsoft.App/managedEnvironments', format('dd-log-forwarder-env-{0}-{1}', parameters('controlPlaneId'), parameters('controlPlaneLocation')))]",
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            },
            {
              "type": "Microsoft.Authorization/roleAssignments",
              "apiVersion": "2022-04-01",
              "name": "[guid('deployer', parameters('controlPlaneId'))]",
              "properties": {
                "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                "roleDefinitionId": "[resourceId('Microsoft.Authorization/roleDefinitions', 'de139f84-1756-47ae-9be6-808fbbe84772')]",
                "principalId": "[reference(resourceId('Microsoft.App/jobs', variables('deployerTaskName')), '2024-03-01', 'full').identity.principalId]"
              },
              "dependsOn": [
                "[resourceId('Microsoft.App/jobs', variables('deployerTaskName'))]"
              ]
            },
            {
              "type": "Microsoft.ManagedIdentity/userAssignedIdentities",
              "apiVersion": "2023-01-31",
              "name": "initialRunIdentity",
              "location": "[parameters('controlPlaneLocation')]"
            },
            {
              "type": "Microsoft.Authorization/roleDefinitions",
              "apiVersion": "2022-04-01",
              "name": "[guid(variables('containerAppStartRoleName'))]",
              "properties": {
                "roleName": "[variables('containerAppStartRoleName')]",
                "description": "Custom role to start container app jobs",
                "type": "customRole",
                "permissions": [
                  {
                    "actions": [
                      "Microsoft.App/jobs/start/action"
                    ]
                  }
                ],
                "assignableScopes": [
                  "[resourceGroup().id]"
                ]
              }
            },
            {
              "type": "Microsoft.Authorization/roleAssignments",
              "apiVersion": "2022-04-01",
              "name": "[guid('initialRunIdentityRoleAssignment', parameters('controlPlaneResourceGroupName'))]",
              "properties": {
                "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                "roleDefinitionId": "[resourceId('Microsoft.Authorization/roleDefinitions', guid(variables('containerAppStartRoleName')))]",
                "principalId": "[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', 'initialRunIdentity'), '2023-01-31').principalId]",
                "principalType": "ServicePrincipal"
              },
              "dependsOn": [
                "[resourceId('Microsoft.Authorization/roleDefinitions', guid(variables('containerAppStartRoleName')))]",
                "[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', 'initialRunIdentity')]"
              ]
            },
            {
              "type": "Microsoft.Resources/deploymentScripts",
              "apiVersion": "2023-08-01",
              "name": "initialRun",
              "location": "[parameters('controlPlaneLocation')]",
              "kind": "AzureCLI",
              "identity": {
                "type": "UserAssigned",
                "userAssignedIdentities": {
                  "[format('{0}', resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', 'initialRunIdentity'))]": {}
                }
              },
              "properties": {
                "storageAccountSettings": {
                  "storageAccountName": "[format('lfostorage{0}', parameters('controlPlaneId'))]",
                  "storageAccountKey": "[listKeys(resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId'))), '2019-06-01').keys[0].value]"
                },
                "azCliVersion": "2.65.0",
                "scriptContent": "[variables('$fxv#0')]",
                "timeout": "PT30M",
                "retentionInterval": "PT1H",
                "cleanupPreference": "OnSuccess"
              },
              "dependsOn": [
                "[resourceId('Microsoft.Authorization/roleAssignments', guid('deployer', parameters('controlPlaneId')))]",
                "[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', 'initialRunIdentity')]",
                "[resourceId('Microsoft.Authorization/roleAssignments', guid('initialRunIdentityRoleAssignment', parameters('controlPlaneResourceGroupName')))]",
                "[resourceId('Microsoft.Storage/storageAccounts', format('lfostorage{0}', parameters('controlPlaneId')))]"
              ]
            }
          ],
          "outputs": {
            "resourceTaskPrincipalId": {
              "type": "string",
              "value": "[reference(resourceId('Microsoft.Web/sites', variables('resourceTaskName')), '2022-09-01', 'full').identity.principalId]"
            },
            "diagnosticSettingsTaskPrincipalId": {
              "type": "string",
              "value": "[reference(resourceId('Microsoft.Web/sites', variables('diagnosticSettingsTaskName')), '2022-09-01', 'full').identity.principalId]"
            },
            "scalingTaskPrincipalId": {
              "type": "string",
              "value": "[reference(resourceId('Microsoft.Web/sites', variables('scalingTaskName')), '2022-09-01', 'full').identity.principalId]"
            }
          }
        }
      },
      "dependsOn": [
        "controlPlaneResourceGroup",
        "validateAPIKey"
      ]
    },
    "subscriptionPermissions": {
      "copy": {
        "name": "subscriptionPermissions",
        "count": "[length(json(parameters('monitoredSubscriptions')))]"
      },
      "type": "Microsoft.Resources/deployments",
      "apiVersion": "2022-09-01",
      "name": "[format('subscriptionPermissions-{0}-{1}', __bicep.subUuid(json(parameters('monitoredSubscriptions'))[copyIndex()]), variables('controlPlaneId'))]",
      "subscriptionId": "[json(parameters('monitoredSubscriptions'))[copyIndex()]]",
      "location": "[deployment().location]",
      "properties": {
        "expressionEvaluationOptions": {
          "scope": "inner"
        },
        "mode": "Incremental",
        "parameters": {
          "resourceGroupName": {
            "value": "[parameters('controlPlaneResourceGroupName')]"
          },
          "location": {
            "value": "[parameters('controlPlaneLocation')]"
          },
          "controlPlaneId": {
            "value": "[variables('controlPlaneId')]"
          },
          "resourceTaskPrincipalId": {
            "value": "[reference('controlPlane').outputs.resourceTaskPrincipalId.value]"
          },
          "diagnosticSettingsTaskPrincipalId": {
            "value": "[reference('controlPlane').outputs.diagnosticSettingsTaskPrincipalId.value]"
          },
          "scalingTaskPrincipalId": {
            "value": "[reference('controlPlane').outputs.scalingTaskPrincipalId.value]"
          }
        },
        "template": {
          "$schema": "https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#",
          "contentVersion": "1.0.0.0",
          "metadata": {
            "_generator": {
              "name": "bicep",
              "version": "0.33.93.31351",
              "templateHash": "1182549309345415189"
            }
          },
          "parameters": {
            "resourceGroupName": {
              "type": "string"
            },
            "location": {
              "type": "string"
            },
            "controlPlaneId": {
              "type": "string"
            },
            "resourceTaskPrincipalId": {
              "type": "string"
            },
            "diagnosticSettingsTaskPrincipalId": {
              "type": "string"
            },
            "scalingTaskPrincipalId": {
              "type": "string"
            }
          },
          "resources": [
            {
              "type": "Microsoft.Resources/resourceGroups",
              "apiVersion": "2021-04-01",
              "name": "[parameters('resourceGroupName')]",
              "location": "[parameters('location')]"
            },
            {
              "type": "Microsoft.Authorization/roleAssignments",
              "apiVersion": "2022-04-01",
              "name": "[guid(subscription().id, 'resourceTask', parameters('controlPlaneId'))]",
              "properties": {
                "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                "roleDefinitionId": "[subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '43d0d8ad-25c7-4714-9337-8ba259a9fe05')]",
                "principalId": "[parameters('resourceTaskPrincipalId')]"
              }
            },
            {
              "type": "Microsoft.Authorization/roleAssignments",
              "apiVersion": "2022-04-01",
              "name": "[guid(subscription().id, 'monitor', 'diagnosticSettings', parameters('controlPlaneId'))]",
              "properties": {
                "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                "roleDefinitionId": "[subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '749f88d5-cbae-40b8-bcfc-e573ddc772fa')]",
                "principalId": "[parameters('diagnosticSettingsTaskPrincipalId')]"
              }
            },
            {
              "type": "Microsoft.Resources/deployments",
              "apiVersion": "2022-09-01",
              "name": "[format('resourceGroupPermissions-{0}', parameters('controlPlaneId'))]",
              "resourceGroup": "[parameters('resourceGroupName')]",
              "properties": {
                "expressionEvaluationOptions": {
                  "scope": "inner"
                },
                "mode": "Incremental",
                "parameters": {
                  "controlPlaneId": {
                    "value": "[parameters('controlPlaneId')]"
                  },
                  "diagnosticSettingsTaskPrincipalId": {
                    "value": "[parameters('diagnosticSettingsTaskPrincipalId')]"
                  },
                  "scalingTaskPrincipalId": {
                    "value": "[parameters('scalingTaskPrincipalId')]"
                  }
                },
                "template": {
                  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
                  "contentVersion": "1.0.0.0",
                  "metadata": {
                    "_generator": {
                      "name": "bicep",
                      "version": "0.33.93.31351",
                      "templateHash": "4358054821976282282"
                    }
                  },
                  "parameters": {
                    "controlPlaneId": {
                      "type": "string"
                    },
                    "diagnosticSettingsTaskPrincipalId": {
                      "type": "string"
                    },
                    "scalingTaskPrincipalId": {
                      "type": "string"
                    }
                  },
                  "resources": [
                    {
                      "type": "Microsoft.Authorization/roleAssignments",
                      "apiVersion": "2022-04-01",
                      "name": "[guid(subscription().id, 'storage', 'diagnosticSettings', parameters('controlPlaneId'))]",
                      "properties": {
                        "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                        "roleDefinitionId": "[resourceId('Microsoft.Authorization/roleDefinitions', 'c12c1c16-33a1-487b-954d-41c89c60f349')]",
                        "principalId": "[parameters('diagnosticSettingsTaskPrincipalId')]"
                      }
                    },
                    {
                      "type": "Microsoft.Authorization/roleAssignments",
                      "apiVersion": "2022-04-01",
                      "name": "[guid(subscription().id, 'scaling', parameters('controlPlaneId'))]",
                      "properties": {
                        "description": "[format('ddlfo{0}', parameters('controlPlaneId'))]",
                        "roleDefinitionId": "[resourceId('Microsoft.Authorization/roleDefinitions', 'b24988ac-6180-42a0-ab88-20f7382dd24c')]",
                        "principalId": "[parameters('scalingTaskPrincipalId')]"
                      }
                    }
                  ]
                }
              },
              "dependsOn": [
                "[subscriptionResourceId('Microsoft.Resources/resourceGroups', parameters('resourceGroupName'))]"
              ]
            }
          ]
        }
      },
      "dependsOn": [
        "controlPlane"
      ]
    }
  }
}
